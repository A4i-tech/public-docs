{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"onboarding/","title":"\ud83d\udc4b Welcome to the A4I Development Team","text":"<p>We\u2019re excited to have you as part of our open-source vision to create social good through technology!  </p> <p>This guide covers: 1. General guidelines &amp; expectations 2. Communications setup 3. Development environment 4. A4I impact areas (Healthcare, Education, Accessibility)</p>"},{"location":"onboarding/#1-general-guidelines","title":"1. \u2705 General Guidelines","text":""},{"location":"onboarding/#tech-proficiency","title":"Tech Proficiency","text":"<p>While A4I has impact across multiple sectors through solutions, we expect new contributors to be comfortable with: - Version Control: Git, GitHub (PRs, Issues, branching) - Programming Basics: Concepts of OS, Networking, Data Structures and Algorithm - Collaboration Tools: Markdown, documentation practices, AI / non-AI based IDEs - Cloud &amp; Infra (Basics):  Docker, CI/CD concepts via Github Actions, knowledge of working on cloud -  Microsoft Azure, AWS or GCP</p> <p>\u26a0\ufe0f If you\u2019re unfamiliar with any of these, don\u2019t worry\u2014pairing and resource links are available.</p>"},{"location":"onboarding/#values-we-uphold","title":"Values We Uphold","text":"<ul> <li>Open-source collaboration</li> <li>Accessibility-first design</li> <li>Clear communication &amp; documentation</li> <li>Regular sprint planning and retrospectives</li> </ul>"},{"location":"onboarding/#2-communications-setup","title":"2. \ud83d\udcac Communications Setup","text":"<ol> <li>Email Access </li> <li>Ensure you have your <code>@iiitb.ac.in</code> email account set up with Outlook access.</li> <li> <p>Add it to GitHub for repo access + notifications.</p> </li> <li> <p>Microsoft Teams </p> </li> <li>Join the <code>A4I IIITB</code> team.  </li> <li>Channels include:  <ul> <li><code>General</code> \u2013 Org-wide updates  </li> <li><code>Engineering Huddle</code> \u2013 Technical discussions  </li> <li><code>Sikshana Foundation</code>, <code>Khushi Baby</code>, <code>Vision Empower</code> \u2013 Sector/project updates with external teams  </li> </ul> </li> </ol>"},{"location":"onboarding/#3-development-environment-setup","title":"3. \ud83d\udcbb Development Environment Setup","text":"<ol> <li>GitHub Access \u2013 Make sure you\u2019re added to the A4I organization.  </li> <li>Clone Repos \u2013 Start with cloning the repository applicable and try to follow the readme steps to get the pre-requisites and the application working.  </li> <li>Install Required Tools:</li> <li>Node.js LTS</li> <li>Python 3.10+</li> <li>Docker Desktop</li> <li>VS Code (recommended editor)</li> <li>Other specific datastores, libraries and tools as suggested in each repository</li> <li>Environment Variables \u2013 Use <code>.env.example</code> files in repos; request actual secrets from admin (never stored in GitHub).  </li> </ol>"},{"location":"onboarding/#4-a4i-impact-areas","title":"4. \ud83c\udf0d A4I Impact Areas","text":""},{"location":"onboarding/#healthcare-impact","title":"\ud83c\udfe5 Healthcare Impact","text":"<ul> <li>\ud83d\udcd1 ASHABot Launch Presentation </li> <li>\ud83d\udcc4 ASHABot Research Paper </li> <li>\ud83d\udcbb Codebase: byoeb</li> </ul>"},{"location":"onboarding/#education-impact","title":"\ud83d\udcda Education Impact","text":"<ul> <li>\ud83d\udcd1 Shiksha Foundation Launch Presentation </li> <li>\ud83d\udcc4 Shiksha Research Paper </li> <li>\ud83d\udcbb Codebase: Shiksha Copilot </li> <li>\ud83c\udfa5 YouTube Training Videos</li> </ul>"},{"location":"onboarding/#accessibility-impact","title":"\u267f Accessibility Impact","text":"<ul> <li>\ud83d\udcd1 Vision Empower Presentation </li> <li>\ud83d\udcc4 Vision Empower Research Paper </li> <li>\ud83d\udcbb Codebase: SEEDS </li> </ul>"},{"location":"onboarding/#need-help","title":"\ud83e\udd1d Need Help?","text":"<p>Reach out to your assigned buddy if you have any queries or run into issues. In case there is no assigned buddy yet, contact @soumabha or @kaltinhoth .  For general issues, you can also drop an email to <code>a4i@iiitb.ac.in</code></p>"},{"location":"workflows/","title":"\ud83d\udd04 Workflows","text":""},{"location":"workflows/#git-workflow","title":"Git Workflow","text":"<ul> <li>Branch naming: Since a4i does a lot of cross collaboration with different agencies - the branch naming convention starts with the org-name, followed by the individual-name, the type of task (<code>feature/*</code>. <code>bugfix/*</code>) and a short description. Example: <code>a4i/soumabha/feature/auth-changes</code></li> <li>Master or Main Branch: a4i/main is usually the main branch across all the codebase repositories. Direct fastforward merge / modification of commit history is not permitted on this branch and has special protected privilege</li> <li>Contribution: to contribute to changes, a new branch must be created from the latest master (a4i/main) and a pull request is to be used to merge changes.</li> <li>Compliance and Testing: Every pull request requires at least 1 reviewer to sign off to the changes. The requestor is also expected to pass all the necessary github actions workflow like secret-detection, unit-testing, code and security scanning.</li> </ul>"},{"location":"workflows/#issue-tracking","title":"Issue Tracking","text":"<ul> <li>Uses GitHub Issues</li> <li>Planning is via Project board</li> <li>Project Labels: <code>SEEDS</code>, <code>Shiksha-Copilot</code>, <code>ASHABot</code>, <code>General</code> : to indicate area of impact.</li> <li>Labels: <code>bug</code>, <code>enhancement</code>, <code>retro</code>, <code>infra</code>: to indicate quality and volume.</li> </ul>"},{"location":"workflows/#cicd","title":"CI/CD","text":"<ul> <li>GitHub Actions run lint/tests on PR</li> <li>Staging deployment happens via a4i/main branch</li> </ul>"},{"location":"architecture/Inclusive-Architecture/","title":"Inclusive Agentic Architecture Template","text":"<p>Building Inclusive, Modular, and Agentic Platforms for Education, Healthcare, and Accessibility</p>"},{"location":"architecture/Inclusive-Architecture/#1-abstract","title":"1. Abstract","text":"<p>This whitepaper presents an inclusive, agentic architecture designed to enable scalable, modular, and human-centred AI systems across domains such as education, healthcare, and accessibility.</p> <p>Current digital ecosystems are fragmented, leading to duplication, limited reuse, and difficulty in scaling innovations. The proposed framework unifies these efforts through a layered reference architecture \u2014 comprising integration channels, experience orchestration, business logic, agent libraries, and tool adapters under an open and extensible model.</p> <p>The architecture emphasizes inclusivity, ensuring accessibility via multimodal channels such as mobile, feature phones, and assistive devices.  It also embeds security, auditability, and compliance guardrails upfront.</p>"},{"location":"architecture/Inclusive-Architecture/#2-motivation","title":"2. Motivation","text":"<ul> <li>Current solutions are point-specific and lack a unified reference model.  </li> <li>Fragmentation challenge: Similar patterns solved differently \u2192 difficult to reuse, scale, or maintain.  </li> <li>Need for a common architectural template that:</li> <li>Reduces duplication of effort across projects.  </li> <li>Ensures scalability &amp; reliability during usage surges.  </li> <li>Simplifies integration of new datastores and connectors.  </li> <li>Embeds security, auditability, and compliance guardrails upfront.  </li> </ul> <p>Long-term Vision: A community-driven reference architecture that improves developer onboarding, speeds up experimentation, and ensures enterprise-grade resilience.</p>"},{"location":"architecture/Inclusive-Architecture/#3-high-level-layered-architecture","title":"3. High-Level Layered Architecture","text":"<p> Figure 1: High-Level Agentic Architecture Template</p>"},{"location":"architecture/Inclusive-Architecture/#4-components-of-the-architecture","title":"4. Components of the Architecture","text":""},{"location":"architecture/Inclusive-Architecture/#41-integration-layer","title":"4.1 Integration Layer","text":"<p>Serves as the system\u2019s backbone for connectivity and interoperability. It manages authentication, auditing, and compliance across integrations. Main goal: make external data feel internal while maintaining isolation and control.</p>"},{"location":"architecture/Inclusive-Architecture/#channels-for-access","title":"Channels for Access","text":"<p>Supports: - Mobile applications - Feature phone\u2013based telecommunications - Web interfaces - Assistive devices (e.g., Hexis, Iris) - Messaging interfaces (WhatsApp, Telegram, SMS)</p>"},{"location":"architecture/Inclusive-Architecture/#data-flow-example","title":"Data Flow Example","text":"<pre><code>sequenceDiagram\n    participant User\n    participant ChannelAdapter\n    participant IntegrationLayer\n    participant ExperienceLayer\n\n    User-&gt;&gt;ChannelAdapter: Message / Event\n    ChannelAdapter-&gt;&gt;IntegrationLayer: Normalized request\n    IntegrationLayer-&gt;&gt;ExperienceLayer: Authenticated, enriched payload</code></pre>"},{"location":"architecture/Inclusive-Architecture/#experience-layer-with-user-conversation-and-memory","title":"Experience Layer (with User, Conversation and Memory)","text":"<p>Acts as the central user interaction entry point, normalizing inputs from diverse channels for unified downstream handling. It enriches the data with context, authentication, and personalization cues, and formats output for the appropriate channel.</p>"},{"location":"architecture/Inclusive-Architecture/#example-data-flow","title":"Example Data Flow","text":""},{"location":"architecture/Inclusive-Architecture/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant User\n    participant IntegrationLayer\n    participant ExperienceLayer\n    participant BusinessOrchestration\n\n    User-&gt;&gt;IntegrationLayer: Sends message via channel adapter &lt;br/&gt;(text, audio, button click)\n    IntegrationLayer-&gt;&gt;ExperienceLayer: Enriched, authenticated input\n    Note right of ExperienceLayer: Create &lt;br/&gt;`standard_request` JSON &lt;br/&gt;with metadata:&lt;br/&gt;\u2022 channel_type&lt;br/&gt;\u2022 user_id&lt;br/&gt;\u2022 locale&lt;br/&gt;\u2022 timestamp&lt;br/&gt;\u2022 raw_content\n    ExperienceLayer-&gt;&gt;BusinessOrchestration: Forward structured request&lt;br/&gt;via REST or Message Bus</code></pre> <p>Sample json <pre><code>{\n  \"standard_request\": {\n    \"channel_type\": \"whatsapp\",\n    \"user_id\": \"ASHA_908\",\n    \"locale\": \"bn\",\n    \"timestamp\": \"2025-10-08T08:20:30Z\",\n    \"content\": \"What are the symptoms of dengue?\"\n  }\n}\n</code></pre></p>"},{"location":"architecture/Inclusive-Architecture/#example-memory-query","title":"Example Memory Query","text":"<pre><code>{\n  \"user_id\": \"ASHA_908\",\n  \"context_type\": \"conversation_summary\"\n}\n</code></pre> <p>Response: <pre><code>{\n  \"last_session_date\": \"2025-10-01\",\n  \"topics_discussed\": [\"child vaccination\", \"vaccine application\"],\n  \"pending_action\": \"confirm new date\",\n  \"raw_messages\": [{\n       ...\n       \"source\": \"whatsapp\",\n       ...\n   }]\n}\n</code></pre></p> <p>The Experience Layer forwards this normalized structure to the Business Orchestration Layer.</p>"},{"location":"architecture/Inclusive-Architecture/#42-business-agentic-orchestration-layer","title":"4.2 Business &amp; Agentic Orchestration Layer","text":"<p>Implements domain-level logic through a hybrid of deterministic services and agentic orchestration.</p> <ul> <li>Business Services: Integrate user information, manage persistent data, and handle workflows.  </li> <li>Agent-Oriented Orchestration: Delegates reasoning and transformation tasks to agents.</li> </ul> <p>In effect, it acts as the \u201cbrain\u201d for domain behaviour, handling branching logic, fallback, error handling, and domain-specific rules or constraints.</p>"},{"location":"architecture/Inclusive-Architecture/#agent-library","title":"Agent Library","text":"<p>Agents operate as autonomous reasoning nodes that receive and emit structured events.  Each agent processes data and can be chained in different order enabling concurrent reasoning and flexible delegation without direct coupling.</p> <p>Encapsulates AI reasoning and cognitive tasks into reusable \"agents\", allowing complex system logic to be broken into small, composable and reusable \"units of work\".</p> Agent Type Function Example Use Case RAG Retrieval-Augmented Generation Query knowledge base Translation Multilingual Interaction Local language conversion ASR/TTS Speech Processing IVR / audio channels Consensus Evaluator Group agreement logic Feedback loops Data Sanity Validation and safety Input consistency <p>Each agent defines input/output contracts and performance expectations for interoperability.</p> <p>Lifecycle and Agent Invocation</p> <p>Step 1: Registered in the central Agent Registry under a versioned identifier.</p> <p>Step 2: Invoked by an Orchestrator or another agent based on workflow logic.</p> <p>Step 3: Executed with full context (user/session/memory data).</p> <p>Step 4: Returns standardized output for downstream agents or final rendering.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Orchestrator\n    participant AgentRegistry\n    participant RAGAgent\n    participant TTSAgent\n\n    Orchestrator-&gt;&gt;AgentRegistry: Lookup(\"rag_agent:v1\")\n    AgentRegistry--&gt;&gt;Orchestrator: Return config &amp; schema\n    Orchestrator-&gt;&gt;RAGAgent: Execute(query=\"symptoms of dengue\")\n    RAGAgent--&gt;&gt;Orchestrator: Return summary + confidence\n    Orchestrator-&gt;&gt;TTSAgent: Convert text \u2192 speech\n    TTSAgent--&gt;&gt;Orchestrator: Return audio file reference\n</code></pre>"},{"location":"architecture/Inclusive-Architecture/#tool-library","title":"Tool Library","text":"<p>Tools are deterministic connectors or adapters that perform actions or I/O without reasoning.</p> Tool Purpose Example WhatsApp Adapter Messaging Send notifications IVR Gateway Telephony Play prompts Email Tool Communication Send updates CRM Adapter External Systems Update lead data MongoDB Adapter Storage Persist structured data <pre><code>sequenceDiagram\n    autonumber\n    participant Agent\n    participant ToolRegistry\n    participant WhatsAppAdapter\n    participant ExternalAPI\n\n    Agent-&gt;&gt;ToolRegistry: Lookup(\"whatsapp_adapter:v2\")\n    ToolRegistry--&gt;&gt;Agent: Return schema + endpoint\n    Agent-&gt;&gt;WhatsAppAdapter: Send(recipient=\"+91XXXX\", template=\"notify_schedule\")\n    WhatsAppAdapter-&gt;&gt;ExternalAPI: POST /messages\n    ExternalAPI--&gt;&gt;WhatsAppAdapter: {status: \"success\", id: \"wa_001\"}\n    WhatsAppAdapter--&gt;&gt;Agent: status=success, timestamp</code></pre>"},{"location":"architecture/Inclusive-Architecture/#example-tool-invocation","title":"Example Tool Invocation","text":"<pre><code>{\n  \"tool\": \"whatsapp_adapter\",\n  \"action\": \"send_message\",\n  \"recipient\": \"+91XXXXXXXXXX\",\n  \"message\": \"Your vaccination is scheduled for 12 Oct at 10 AM.\",\n  \"template_id\": \"notify_schedule\"\n}\n</code></pre> <p>Real World Example: Sahayak Orchestrator</p> <p>The Sahayak Orchestrator exemplifies how these architectural layers operate in unison.  - It receives user inputs\u2014along with historical context and session memory\u2014and determines which agents or tools should act next.  - Using its centralized Tool Registry, populated with system-level MCP (Multi-Channel Plugin) abstractions, the orchestrator dynamically routes requests to the appropriate agents for reasoning and to the corresponding tools for deterministic execution.  - This event-driven workflow enables Sahayak to coordinate human-like reasoning with system-level precision, ensuring adaptive, multimodal interactions across channels and services.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant ExperienceLayer\n    participant Orchestrator\n    participant Agent\n    participant Tool\n    participant ExternalService\n\n    ExperienceLayer-&gt;&gt;Orchestrator: User prompt, context or event data\n    Orchestrator-&gt;&gt;Agent: SahayakTextRequestEvent\n    Agent-&gt;&gt;Tool: ToolCallRequestEvent (resolved via known_tool_owners)\n    Tool-&gt;&gt;ExternalService: Executes operation (e.g., send message)\n    ExternalService--&gt;&gt;Tool: Returns API result\n    Tool--&gt;&gt;Agent: ToolCallExecutionEvent (structured response)\n    Agent--&gt;&gt;Orchestrator: Final message event\n    Orchestrator--&gt;&gt;ExperienceLayer: Rendered response</code></pre>"},{"location":"architecture/Inclusive-Architecture/#43-ingestion-layer","title":"4.3 Ingestion Layer","text":"<p>converts raw or semi-structured content (textbooks, media, logs) into enriched, searchable knowledge representations for downstream agents. Supports high-throughput ingestion, context caching, and long-term knowledge storage.</p>"},{"location":"architecture/Inclusive-Architecture/#data-flow_1","title":"Data Flow","text":"<pre><code>flowchart LR\n    A[1\ufe0f\u20e3 Ingestion Trigger] --&gt; B[2\ufe0f\u20e3 Data Retrieval]\n    B --&gt; C[3\ufe0f\u20e3 Parsing &amp; Preprocessing]\n    C --&gt; D[4\ufe0f\u20e3 Chunking &amp; Metadata Enrichment]\n    D --&gt; E[5\ufe0f\u20e3 Embedding &amp; Indexing]\n    E --&gt; F[6\ufe0f\u20e3 Storage &amp; Caching]\n    F --&gt; G[7\ufe0f\u20e3 Event Logging &amp; Completion]</code></pre> <p>Ingestion layer relies heavily on the agent and tool libraries preprovisioned. Sample examples</p> Type Responsibility Example Components Parser Agent Handles parsing, cleaning, extraction logic. <code>TextExtractionAgent</code>, <code>SpeechToTextAgent</code> Chunking Agent Segments and structures data. <code>ChunkingAgent</code>, <code>ContentSplitterTool</code> Embedding Agent Generates semantic embeddings from processed text. <code>EmbeddingAgent</code>, <code>OpenAIEmbeddingTool</code> Storage / Memory Tools Persist processed data into memory or vector DBs. <code>VectorDBTool</code>, <code>KnowledgeBaseAdapter</code>, <code>CacheTool</code> Logging &amp; Audit Agents Maintain ingestion logs, telemetry, and versioned lineage. <code>IngestionLoggerAgent</code>, <code>TelemetryCollectorTool</code>"},{"location":"architecture/Inclusive-Architecture/#5-real-world-applications","title":"5. Real-World Applications","text":""},{"location":"architecture/Inclusive-Architecture/#education","title":"Education","text":"<ul> <li>Teachers can aggregate lesson plans, generate evaluations, and access learning content through multilingual and multimodal channels.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#healthcare","title":"Healthcare","text":"<ul> <li>Field health workers (ASHAs/ANMs) can report via WhatsApp or IVR, enabling data capture and alerts in remote areas.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#accessibility","title":"Accessibility","text":"<ul> <li>Voice and assistive-device-based interfaces ensure participation for users with disabilities or low-tech environments.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#6-design-principles","title":"6. \ud83c\udfaf Design Principles","text":"\ud83e\udde9 Principle \ud83d\udca1 Description \ud83d\udd17 Implication \ud83c\udf10 Open Source First Built to evolve collaboratively and transparently. Encourages community contributions and shared innovation. \ud83e\uddf1 Highly Modular Each capability functions as a reusable component or library. Simplifies updates, enables team-specific extensions. \u2696\ufe0f Horizontally Scalable Designed for distributed, cloud-native scaling. Supports large concurrent workloads and adaptive scaling. \u267f Inclusive &amp; Multimodal Accessible via text, voice, and assistive devices. Ensures equitable access across digital divides. \ud83e\udd16 AI-First Agentic reasoning and cognitive workflows by design. Enables contextual automation and hybrid human-AI work. \ud83d\udd12 Secure by Design Data flow and access control built into architecture. Simplifies compliance, auditability, and data governance. \ud83d\udd0c Plug &amp; Play Extensible via standardized connectors and APIs. Allows rapid onboarding of new channels or services. <p>\ud83d\udcac These principles form the DNA of the Agentic Template \u2014 ensuring it stays modular, scalable, and inclusive across domains.</p>"},{"location":"architecture/Inclusive-Architecture/#7-limitations-future-considerations","title":"7. Limitations &amp; Future Considerations","text":"<ul> <li>Complex Orchestration: Debugging multi-agent workflows may be non-trivial.  </li> <li>Connectivity Dependency: Cloud-dependent features (RAG, ASR/TTS) may face offline limitations.  </li> <li>Tool Explosion: Too many tools risk hallucinated abilities. Guardrails and observability layers are essential.  </li> <li>Maintenance Overhead: Hybrid agentic-service systems require continuous synchronization and version management.</li> <li>Evolving Standards: Interoperability may drift with external APIs and / or AI model changes</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#8-references","title":"8. References","text":"<ul> <li>Inclusive Architecture Whitepaper (2025) </li> <li>Architecture Template (Draft, A4I, Aug 2025) </li> <li>Microsoft Azure AI &amp; Cognitive Services Documentation  </li> <li>OpenAI Function Calling &amp; Orchestration Patterns (2024)  </li> <li>WHO Digital Health Guidelines (2023)</li> </ul> <p>\u00a9 2025 A4I. All Rights Reserved.</p>"},{"location":"architecture/agentic-governance-safety-framework/","title":"Agentic Governance & Safety Framework","text":""},{"location":"architecture/agentic-governance-safety-framework/#purpose-and-scope","title":"Purpose and Scope","text":"<p>This document proposes a common, cross-cutting governance and safety framework that embeds compliance, cost control, and safety into every AI interaction across digital public AI goods:</p> <ul> <li>BYOEB (ASHA WhatsApp RAG for healthcare) \u2014 healthcare compliance</li> <li>Shiksha Copilot (lesson outcome\u2013driven education copilot) \u2014 education compliance</li> <li>SEEDS (audio-first IVR and conferencing for visually impaired learners) \u2014 education + accessibility compliance</li> </ul> <p>\u2026and is extensible to new products, domains, and regulatory requirements.</p> <p>It aligns with the Inclusive Architecture as a horizontal layer that wraps around the Business &amp; Agentic Orchestration Layer \u2014 \"request guard \u2192 consent check \u2192 cost tracking \u2192 LLM call \u2192 response guard \u2192 audit logging\" \u2014 and implements those as reusable agents and tools across domains.</p> <p>\ud83d\udcd6 Related: The LLM call step is abstracted through the LLM Gateway Layer, which provides unified access to multiple LLM providers with built-in cost tracking, failover, and guardrails.</p>"},{"location":"architecture/agentic-governance-safety-framework/#document-navigation","title":"Document Navigation","text":"<ul> <li>New to the system? Start with LLM Gateway Layer to understand foundational infrastructure</li> <li>Understanding integration? See What the Gateway Provides vs What Governance Adds (this document)</li> <li>Implementing Gateway? See LLM Gateway Layer Built-in Features</li> <li>Implementing Governance? See Engine Layer (this document)</li> </ul>"},{"location":"architecture/agentic-governance-safety-framework/#layered-architecture","title":"Layered Architecture","text":""},{"location":"architecture/agentic-governance-safety-framework/#design-drivers","title":"Design Drivers","text":"<ol> <li> <p>Aligned with Inclusive Architecture</p> <ul> <li>Governance is a cross-cutting concern serving multiple domains (healthcare, education, accessibility) via common agents &amp; tools.</li> <li>All products interact with AI through governance-wrapped interfaces; governance itself is multi-tenant and domain-aware.</li> </ul> </li> <li> <p>Multi-Regulation</p> <ul> <li>Healthcare: PHI protection, clinical data handling</li> <li>Education: Student data protection, minor data handling</li> <li>General: Data privacy, security controls</li> <li>AI-Specific: Transparency, accountability, bias prevention</li> </ul> </li> <li> <p>Multi-Concern</p> <ul> <li>Consent &amp; Privacy: Verifiable consent management (parental consent, data processing authorization)</li> <li>Data Protection: PII/PHI detection, anonymization, retention policies</li> <li>Cost Control: Token tracking, budget enforcement, usage optimization</li> <li>Safety: Content moderation, hallucination detection, topic guards</li> <li>Audit: Comprehensive logging, compliance reporting, breach detection</li> </ul> </li> <li> <p>Domain-Aware but Reusable</p> <ul> <li>BYOEB's healthcare compliance (PHI handling) should be a profile on top of generic governance steps.</li> <li>Shiksha's education compliance (parental consent, student data) should be the default education profile.</li> <li>SEEDS' accessibility considerations should extend the education profile.</li> </ul> </li> <li> <p>Non-Blocking &amp; Observable</p> <ul> <li>Governance adds minimal latency to AI interactions.</li> <li>Strong observability for compliance audits and incident response.</li> <li>Human-in-the-loop escalation for edge cases.</li> </ul> </li> </ol>"},{"location":"architecture/agentic-governance-safety-framework/#conceptual-model","title":"Conceptual Model","text":"<p>We define three layers inside the governance system (itself mapped as a horizontal wrapper in the Inclusive Architecture):</p> <ol> <li>Guard Layer: Request and response interception with policy evaluation.</li> <li>Engine Layer: Consent, Cost, Safety, and Audit engines that implement governance logic.</li> <li>Policy &amp; Configuration Layer: Domain profiles, regulatory mappings, and tenant configuration.</li> </ol>"},{"location":"architecture/agentic-governance-safety-framework/#high-level-governance-flow","title":"High-Level Governance Flow","text":"<p>The governance flow maps to the Inclusive Architecture layers while progressively enriching the GovernanceContext\u2014a data structure that flows with each request.</p> Step Description IA Layer Governance Concept Context Field Enriched 1. Request Received User request arrives via channel Integration \u2014 <code>request_id</code>, <code>channel</code> 2. Context Initialization Create GovernanceContext, resolve user/tenant Experience GovernanceContext <code>user_id</code>, <code>tenant_id</code>, <code>domain_profile</code> 3. Consent Resolution Query for user's consent status Experience Engine Layer: Consent <code>consent_status</code> 4. Budget Resolution Query Gateway for spend status Experience LLM Gateway <code>budget_status</code> 5. Request Guards Evaluate consent guards, budget alerts Business &amp; Agentic Guard Layer: Request <code>guard_results</code> 6. Domain PII Handling Detect domain-specific PII (PHI, education) &amp; anonymize Business &amp; Agentic Engine Layer: Safety <code>anonymization_mapping_id</code> 7. LLM Execution Call LLM via Gateway (generic PII + moderation) Business &amp; Agentic LLM Gateway <code>gateway_request_id</code> 8. Response Guards Evaluate output compliance, deanonymize domain PII Business &amp; Agentic Guard Layer: Response <code>response_guard_results</code> 9. Cost Capture Extract cost/tokens from Gateway response Business &amp; Agentic LLM Gateway <code>cost</code>, <code>tokens_used</code>, <code>model_used</code> 10. Audit Logging Log governance-specific events (guards, consent) Business &amp; Agentic Engine Layer: Audit \u2014 (persisted) 11. Response Delivery Return response to user via channel Integration \u2014 \u2014 <p>\ud83d\udcd6 Gateway vs Governance Responsibilities: See What the Gateway Provides vs What Governance Adds below for detailed responsibility boundaries.</p> <p>We implement each step as one or more agents (reasoning/policy-heavy) and tools (IO or deterministic work).</p>"},{"location":"architecture/agentic-governance-safety-framework/#governancecontext-the-canonical-data-model","title":"GovernanceContext: The Canonical Data Model","text":"<p>The <code>GovernanceContext</code> is an in-memory data structure that flows through Inclusive Architecture layers, getting enriched at each step. It is NOT a separate application\u2014rather, it's the \"request context\" for governance operations.</p> Field Description LiteLLM Mapping Storage <code>request_id</code> UUID for this governance request <code>metadata.request_id</code> <code>governance_audit_events</code> <code>correlation_id</code> UUID linking related requests <code>metadata.correlation_id</code> <code>governance_audit_events</code> <code>tenant_id</code> Tenant/partner/org context <code>metadata.team_id</code> (virtual key) <code>governance_audit_events</code> <code>user_id</code> User making the request <code>metadata.user_id</code> (spend tracking) <code>governance_audit_events</code> <code>domain_profile</code> health_byoeb / education_shiksha / etc. <code>metadata.domain_profile</code> <code>governance_audit_events</code> <code>consent_status</code> Map of consent types to status \u2014 <code>consent_records</code> <code>budget_status</code> Current spend vs. limits (queried from Gateway) \u2014 \u2014 (read from <code>LiteLLM_TeamTable</code>) <code>anonymization_mapping_id</code> Reference to domain PII mapping for deanonymization <code>metadata.anon_mapping_id</code> <code>anonymization_mappings</code> <code>gateway_request_id</code> LiteLLM's request ID for correlation Response <code>id</code> \u2014 (stored in <code>LiteLLM_SpendLogs</code>) <code>cost</code> Request cost in USD Response <code>response_cost</code> \u2014 (stored in <code>LiteLLM_SpendLogs</code>) <code>tokens_used</code> Total tokens consumed Response <code>usage.total_tokens</code> \u2014 (stored in <code>LiteLLM_SpendLogs</code>) <code>model_used</code> Actual model that served the request Response <code>model</code> \u2014 (stored in <code>LiteLLM_SpendLogs</code>) <code>guard_results</code> Results from request/response guards \u2014 <code>governance_audit_events</code> <code>audit_metadata</code> Timestamps, IP, user agent, regulations \u2014 <code>governance_audit_events</code> <p>\ud83d\udcdd Storage Principle: Fields with LiteLLM mapping are stored by the Gateway\u2014governance queries them, doesn't duplicate. Governance only stores compliance-specific data (consent, guard decisions, anonymization mappings).</p>"},{"location":"architecture/agentic-governance-safety-framework/#how-the-system-works","title":"How the System Works","text":"<p>1. GovernanceContext flows through Inclusive Architecture layers:</p> <pre><code>flowchart LR\n    subgraph Request Flow\n        A[User Request] --&gt; B[Integration Layer]\n        B --&gt; C[Experience Layer]\n        C --&gt; D[Business &amp; Agentic Layer]\n    end\n    subgraph Governance Engine\n        B --&gt; B1[context created]\n        B1 --\"request_id, channel\" --&gt; B\n        C --&gt; C1[context enriched]\n        C1 --\"user_id, tenant_id, consent_status, budget_status\" --&gt; C\n    end\n    subgraph LLM Gateway \n        D1[+guards + audit] --&gt; E   \n        D --&gt; D1\n        E[LLM Invocation] --&gt; D1\n    end</code></pre> <p>2. Governance Engines are stateful services the context interacts with:</p> <pre><code>flowchart TB\n    subgraph Governance Tools\n        CE[ConsentEngine&lt;br/&gt;getStatus, request, verify]\n        COE[CostEngine&lt;br/&gt;getBudget, alert]\n        SE[SafetyEngine&lt;br/&gt;anonymize, deanonymize]\n        AE[AuditEngine&lt;br/&gt;log, query, export]\n    end\n\n    subgraph Storage\n        PG[(PostgreSQL&lt;br/&gt;Governance Schema)]\n    end\n\n    CE --&gt; PG\n    COE --&gt; PG\n    SE --&gt; PG\n    AE --&gt; PG</code></pre> <p>3. LLM Gateway receives context as metadata:</p> <pre><code>sequenceDiagram\n    participant GC as GovernanceContext\n    participant GW as LLM Gateway (LiteLLM)\n    participant LLM as LLM Provider\n\n    GC-&gt;&gt;GW: completion(model, messages, metadata)\n    Note right of GC: metadata = {&lt;br/&gt;request_id,&lt;br/&gt;team_id (tenant),&lt;br/&gt;user_id,&lt;br/&gt;domain_profile&lt;br/&gt;}\n    GW-&gt;&gt;LLM: Routed request\n    LLM--&gt;&gt;GW: Response + usage\n    GW--&gt;&gt;GC: Response enriches context\n    Note right of GC: context.cost = response_cost&lt;br/&gt;context.tokens = total_tokens&lt;br/&gt;context.gateway_id = response.id</code></pre> <p>Summary: GovernanceContext is a data structure that flows with the request through Inclusive Architecture layers. It interacts with Governance Engines (stateful services) to resolve consent, budget, and safety status, and passes relevant fields to the LLM Gateway as metadata. After the LLM call, cost and usage data from the Gateway enriches the context before final audit logging.</p>"},{"location":"architecture/agentic-governance-safety-framework/#core-implementation","title":"Core Implementation","text":"<p>\u26a0\ufe0f Implementation Layering: This framework builds ON TOP of the LLM Gateway Layer. The gateway provides foundational capabilities (unified LLM API, cost tracking, generic PII detection, content moderation, rate limiting, observability). This governance framework adds domain-specific compliance logic that the gateway cannot provide: consent management, regulatory audit trails, PHI/education-specific guards, and compliance reporting.</p>"},{"location":"architecture/agentic-governance-safety-framework/#what-the-gateway-provides-vs-what-governance-adds","title":"What the Gateway Provides vs What Governance Adds","text":"Capability LLM Gateway (LiteLLM) Governance Framework LLM routing &amp; failover \u2705 Built-in \u2014 Cost calculation \u2705 Per-request automatic Compliance cost reports Budget enforcement \u2705 Per-key/team blocking Alert thresholds, compliance attribution Generic PII detection \u2705 Presidio integration \u2014 Domain PII (PHI, education) \u274c \u2705 Domain-specific guards Content moderation \u2705 OpenAI/LlamaGuard \u2014 Prompt injection \u2705 Lakera integration \u2014 Topic restrictions \u274c Generic only \u2705 Domain-specific topic lists Age-appropriate content \u274c \u2705 Minor data protection guards Consent management \u274c \u2705 Domain-specific consent LLM call logging \u2705 Full metadata \u2014 Governance events \u274c \u2705 Consent, guard decisions Compliance reports \u274c \u2705 Regulatory exports Retention policies \u274c \u2705 Configurable per domain"},{"location":"architecture/agentic-governance-safety-framework/#guard-layer","title":"Guard Layer","text":"<p>Request Guard:</p> <p>Guards intercept requests before they reach the agentic layer. A guard evaluates a request and returns one of: - <code>ALLOW</code> \u2014 Request proceeds unchanged - <code>BLOCK</code> \u2014 Request is rejected with reason - <code>MODIFY</code> \u2014 Request content is transformed (e.g., PII anonymized) - <code>ESCALATE</code> \u2014 Request requires human review</p> <pre><code>interface RequestGuard {\n  evaluate(request: GovernanceRequest): Promise&lt;GuardResult&gt;;\n}\n\ninterface GuardResult {\n  decision: 'allow' | 'block' | 'modify' | 'escalate';\n  reason?: string;\n  modifiedContent?: string;\n  violations?: string[];\n}\n</code></pre> <p>Guard Types:</p> <ol> <li> <p>Consent Guards (Governance-specific \u2014 not in gateway)</p> <ul> <li><code>ParentalConsentGuard</code> \u2014 Verify parental consent for minors</li> <li><code>DataProcessingAuthorizationGuard</code> \u2014 Verify data processing authorization</li> <li><code>DisclosureConsentGuard</code> \u2014 Verify data disclosure consent</li> </ul> </li> <li> <p>Safety Guards (Mixed \u2014 some leverage gateway, some governance-specific)</p> <ul> <li><code>PIIDetectionGuard</code> \u2014 Delegates to gateway's Presidio integration</li> <li><code>PHIFilterGuard</code> \u2014 Governance-specific: healthcare PHI beyond generic PII</li> <li><code>ContentModerationGuard</code> \u2014 Delegates to gateway's OpenAI/LlamaGuard integration</li> <li><code>TopicRestrictionGuard</code> \u2014 Governance-specific: domain-aware topic filtering</li> <li><code>MinorDataGuard</code> \u2014 Governance-specific: age-appropriate content for minors</li> </ul> </li> <li> <p>Cost Guards (Leverage gateway with governance extensions)</p> <ul> <li><code>BudgetGuard</code> \u2014 Queries gateway's spend tracking + governance alert thresholds</li> <li><code>RateLimitGuard</code> \u2014 Delegates to gateway's per-key rate limiting</li> </ul> </li> </ol> <p>Response Guard:</p> <p>Guards validate AI-generated responses before returning to users.</p> <ol> <li> <p>Compliance Guards (Governance-specific \u2014 not in gateway)</p> <ul> <li><code>PHIDisclosureGuard</code> \u2014 Prevent unauthorized PHI in responses</li> <li><code>MinorDataGuard</code> \u2014 Extra protection for children's data</li> </ul> </li> <li> <p>Quality Guards (Governance-specific \u2014 domain-aware validation)</p> <ul> <li><code>HallucinationDetectionGuard</code> \u2014 Flag potential fabricated content</li> <li><code>SourceAttributionGuard</code> \u2014 Ensure RAG responses cite sources</li> </ul> </li> <li> <p>Restoration Guards (Governance-specific \u2014 reversible anonymization)</p> <ul> <li><code>DeAnonymizationGuard</code> \u2014 Restore original values using stored mapping</li> </ul> </li> </ol>"},{"location":"architecture/agentic-governance-safety-framework/#engine-layer","title":"Engine Layer","text":"<p>1. Consent Engine</p> <p>Manages user consent across regulatory requirements.</p> <pre><code>interface ConsentEngine {\n  checkConsent(userId, consentType, dataCategory): Promise&lt;boolean&gt;;\n  requestConsent(userId, consentType, scope): Promise&lt;ConsentRequest&gt;;\n  verifyConsent(consentId, verificationData): Promise&lt;boolean&gt;;\n  withdrawConsent(userId, consentType): Promise&lt;void&gt;;\n  getConsentHistory(userId): Promise&lt;ConsentRecord[]&gt;;\n}\n</code></pre> <p>Consent Types: - <code>PARENTAL_CONSENT</code> \u2014 Verifiable parental consent for minors - <code>DISCLOSURE_CONSENT</code> \u2014 Authorization for data disclosure - <code>DATA_PROCESSING_AUTHORIZATION</code> \u2014 Sensitive data processing authorization - <code>AI_PROCESSING</code> \u2014 General AI processing consent - <code>DATA_RETENTION</code> \u2014 Consent for data storage beyond session</p> <p>Parental Consent Verification Methods: - Credit card micro-transaction - Knowledge-based authentication - Government ID verification - Video call verification</p> <p>2. Cost Engine</p> <p>Extends the LLM Gateway Layer's built-in cost tracking with governance-specific capabilities.</p> <p>\ud83d\udcd6 Gateway Provides: Per-request cost calculation, virtual key budgets, spend tracking database, per-team/per-key limits. See LLM Gateway Layer Cost Tracking for implementation details and code examples.</p> <p>Governance Framework Adds:</p> <pre><code>interface GovernanceCostEngine {\n  // \u2500\u2500 PROACTIVE (before LLM call) \u2500\u2500\n  // Query gateway for current spend, trigger warning alerts\n  checkBudgetStatus(tenantId): Promise&lt;BudgetStatus&gt;;\n  checkAlertThreshold(tenantId): Promise&lt;AlertStatus&gt;;\n\n  // \u2500\u2500 REACTIVE (after LLM call / on-demand) \u2500\u2500\n  // Query gateway data for compliance reporting (no duplication)\n  queryGatewaySpend(tenantId, dateRange): Promise&lt;SpendData&gt;;\n  getUsageSummary(tenantId, domainProfile?, dateRange): Promise&lt;UsageSummary&gt;;\n  generateCostReport(tenantId, regulation, dateRange): Promise&lt;CostReport&gt;;\n\n  // \u2500\u2500 CONFIGURATION \u2500\u2500\n  configureAlerts(tenantId, alertConfig): Promise&lt;void&gt;;\n  getAlertHistory(tenantId): Promise&lt;AlertEvent[]&gt;;\n}\n</code></pre> <p>\ud83d\udd11 Key Principle: Cost Engine QUERIES the Gateway\u2014it does NOT duplicate storage. All cost/usage data lives in <code>LiteLLM_SpendLogs</code>.</p> <p>Proactive vs Reactive Behavior:</p> Behavior When What Happens Proactive Before LLM call Query Gateway spend \u2192 check against <code>governance_alert_config</code> \u2192 send warning if &gt;80% Reactive After LLM call Gateway records cost automatically; governance queries for reports <p>Governance-Specific Extensions:</p> Capability Gateway (LiteLLM) Governance Framework Adds Cost calculation \u2705 Per-request automatic \u2014 (use gateway) Budget enforcement \u2705 Per-key/team blocking Proactive: Alert thresholds before blocking (80% warning) Spend tracking \u2705 PostgreSQL tables \u2014 (query gateway, don't duplicate) Reporting Basic spend queries Reactive: Compliance-formatted reports per domain Attribution Team/key level Reactive: User + domain profile + regulation context in reports <p>3. Safety Engine</p> <p>Extends the LLM Gateway Layer's built-in guardrails with domain-specific safety logic.</p> <p>\ud83d\udcd6 Gateway Provides: Generic PII detection (Presidio), prompt injection protection (Lakera), content moderation (OpenAI Moderation, LlamaGuard), custom guardrail hooks. See LLM Gateway Layer Guardrails for implementation details and configuration examples.</p> <p>Governance Framework Adds:</p> <pre><code>interface GovernanceSafetyEngine {\n  // \u2500\u2500 PROACTIVE (before LLM call) \u2500\u2500\n  // Domain-specific PII beyond gateway's generic detection\n  detectDomainPII(text, domainProfile): Promise&lt;DomainPIIDetection[]&gt;;\n  // Reversible anonymization (gateway's Presidio is one-way)\n  anonymizeWithMapping(text, detections): Promise&lt;{text: string, mapping: AnonymizationMapping}&gt;;\n  // Domain-specific topic compliance\n  checkTopicCompliance(text, domainProfile): Promise&lt;TopicResult&gt;;\n  // Age-appropriate content filtering\n  checkAgeAppropriateness(text, userAge): Promise&lt;AgeComplianceResult&gt;;\n\n  // \u2500\u2500 REACTIVE (after LLM call) \u2500\u2500\n  // Restore original values in response using stored mapping\n  deanonymize(text, mappingId): string;\n  // Check response for PHI disclosure\n  checkResponseCompliance(response, domainProfile): Promise&lt;ComplianceResult&gt;;\n}\n</code></pre> <p>\ud83d\udd11 Key Principle: Safety Engine adds domain-specific detection (PHI, education records) ON TOP of Gateway's generic PII. It also provides reversible anonymization with mapping storage.</p> <p>Proactive vs Reactive Behavior:</p> Behavior When What Happens Proactive Before LLM call Detect domain PII \u2192 anonymize with mapping \u2192 store mapping \u2192 send to Gateway Reactive After LLM call Retrieve mapping \u2192 deanonymize response \u2192 check for unauthorized disclosure <p>Governance-Specific Extensions:</p> Capability Gateway (LiteLLM) Governance Framework Adds Generic PII (name, email, phone) \u2705 Presidio integration \u2014 (use gateway) PHI detection (healthcare identifiers) \u274c Not domain-aware Proactive: <code>PHIFilterGuard</code> with MRN, conditions, medications Education PII (student ID, grades) \u274c Not domain-aware Proactive: <code>EducationDataGuard</code> with education-specific entities Content moderation \u2705 OpenAI/LlamaGuard \u2014 (use gateway) Prompt injection \u2705 Lakera integration \u2014 (use gateway) Topic restrictions \u274c Generic only Proactive: Domain-specific topic lists per profile Age-appropriate content \u274c Not available Proactive: <code>MinorDataGuard</code> for minor data protection Reversible anonymization \u274c One-way only Proactive + Reactive: Mapping storage \u2192 deanonymization Response compliance \u274c Not available Reactive: PHI disclosure check, minor data protection <p>4. Audit Engine</p> <p>Extends the LLM Gateway Layer's logging with compliance-specific audit trails.</p> <p>\ud83d\udcd6 Gateway Provides: Request/response metadata, token counts, costs, latency, model used, error details, callbacks to observability platforms (Langfuse, Prometheus, OpenTelemetry). See LLM Gateway Layer Audit Logging for implementation details and observability setup.</p> <p>Governance Framework Adds:</p> <pre><code>interface GovernanceAuditEngine {\n  // \u2500\u2500 REACTIVE (during/after request processing) \u2500\u2500\n  // Log governance-specific events (not LLM call metadata)\n  logConsentEvent(event: ConsentEvent): Promise&lt;string&gt;;\n  logGuardDecision(context: GovernanceContext, result: GuardResult): Promise&lt;string&gt;;\n  logPIIEvent(event: PIIEvent): Promise&lt;string&gt;;\n\n  // Link governance audit to gateway logs (via gateway_request_id)\n  linkToGatewayLog(governanceEventId, gatewayRequestId): Promise&lt;void&gt;;\n\n  // \u2500\u2500 ON-DEMAND (compliance reporting) \u2500\u2500\n  // Query governance events + gateway logs for unified reports\n  generateComplianceReport(tenantId, regulation, dateRange): Promise&lt;ComplianceReport&gt;;\n  exportForRegulator(tenantId, regulation, dateRange): Promise&lt;RegulatoryExport&gt;;\n\n  // \u2500\u2500 SCHEDULED (background jobs) \u2500\u2500\n  // Retention policy enforcement per regulation\n  enforceRetention(regulation): Promise&lt;RetentionResult&gt;;\n  // Breach detection patterns\n  detectAnomalies(tenantId, dateRange): Promise&lt;AnomalyReport&gt;;\n}\n</code></pre> <p>\ud83d\udd11 Key Principle: Audit Engine logs governance-specific events (consent, guards, PII handling) to <code>governance_audit_events</code>. It links to Gateway logs via <code>gateway_request_id</code> for unified compliance reporting\u2014it does NOT duplicate LLM call data.</p> <p>Proactive vs Reactive Behavior:</p> Behavior When What Happens Reactive During request Log consent checks, guard decisions, PII detection events Reactive After request Link governance event to Gateway log via <code>gateway_request_id</code> On-Demand Compliance audit Query governance events + Gateway logs \u2192 generate unified report Scheduled Background Enforce retention policies, detect anomalies/breaches <p>Governance-Specific Extensions:</p> Capability Gateway (LiteLLM) Governance Framework Adds LLM call logging \u2705 Full request/response \u2014 (use gateway) Token/cost tracking \u2705 Per-request automatic \u2014 (use gateway) Latency metrics \u2705 Built-in \u2014 (use gateway) Observability export \u2705 Langfuse, Prometheus \u2014 (use gateway) Consent events \u274c Not tracked Reactive: <code>CONSENT_CHECKED</code>, <code>CONSENT_GRANTED</code>, <code>CONSENT_WITHDRAWN</code> Guard decisions \u274c Not tracked Reactive: <code>REQUEST_BLOCKED</code>, <code>REQUEST_MODIFIED</code>, <code>ESCALATION_TRIGGERED</code> PII detection events \u274c Not tracked Reactive: <code>PII_DETECTED</code>, <code>PII_ANONYMIZED</code>, <code>PII_DEANONYMIZED</code> Compliance reports \u274c Generic queries only On-Demand: Domain-specific audit reports, disclosure logs Regulatory retention \u274c Not enforced Scheduled: Configurable retention per domain profile Breach detection \u274c Not available Scheduled: Unusual access patterns, unauthorized disclosure alerts"},{"location":"architecture/agentic-governance-safety-framework/#policy-configuration-layer","title":"Policy &amp; Configuration Layer","text":"<p>Domain Profiles:</p> <p>Domain profiles use YAML schema with regulatory mappings and tenant configuration.</p> <pre><code>domain_profile: health_byoeb\ndomain_type: healthcare\npii_types:\n  - name\n  - dob\n  - ssn\n  - mrn\n  - health_condition\n  - medication\nconsent_required: true\nconsent_types:\n  - DATA_PROCESSING_AUTHORIZATION\naudit_level: full\nretention_years: 6\nguards:\n  request:\n    - DataProcessingAuthorizationGuard\n    - PHIFilterGuard\n    - BudgetGuard\n  response:\n    - PHIDisclosureGuard\n    - HallucinationDetectionGuard\n</code></pre> <pre><code>domain_profile: education_shiksha\ndomain_type: education\npii_types:\n  - name\n  - dob\n  - student_id\n  - grades\nage_gate_threshold: 13\nparental_consent_required: true\nconsent_types:\n  - PARENTAL_CONSENT\n  - DISCLOSURE_CONSENT\naudit_level: full\nretention_years: 5\nguards:\n  request:\n    - ParentalConsentGuard\n    - PIIDetectionGuard\n    - TopicRestrictionGuard\n    - BudgetGuard\n  response:\n    - MinorDataGuard\n    - ContentModerationGuard\n</code></pre>"},{"location":"architecture/agentic-governance-safety-framework/#governance-pipeline-orchestration","title":"Governance Pipeline Orchestration","text":"<p>Static Orchestration with Policy-Driven Guards:</p> <p>The governance framework uses declarative, static pipelines \u2014 NOT LLM-driven decision-making for governance. This is deliberate:</p> Aspect Our Approach Alternative (LLM Governance) Guard sequencing Defined in config, runs deterministically LLM decides which guards to apply LLM usage Only within safety engines (e.g., content moderation) Also for policy decisions Predictability High \u2014 same request gets same governance Variable \u2014 LLM may interpret policies differently Auditability Full \u2014 policy version = exact behavior Partial \u2014 depends on LLM reasoning Compliance Provable \u2014 regulators can verify logic Uncertain \u2014 LLM decisions are opaque <p>Why static orchestration?</p> <ul> <li>Governance requires predictability \u2014 the same request must be treated the same way.</li> <li>Compliance audits require provable behavior \u2014 we must show exactly what logic was applied.</li> <li>Guards are already policy-rich \u2014 they encode regulatory requirements, not general reasoning.</li> <li>Cost-efficient: No reasoning overhead for clear-cut compliance decisions.</li> </ul> <p>Example pipeline configuration:</p> <pre><code>governance_pipeline: byoeb_healthcare_v1\ndomain_profile: health_byoeb\nrequest_guards:\n  - guard: DataProcessingAuthorizationGuard\n    config: { authorization_types: [\"treatment\", \"payment\", \"operations\"] }\n  - guard: PHIFilterGuard\n    config: { anonymize: true, hash_algorithm: \"sha256\" }\n  - guard: BudgetGuard\n    config: { check_user: true, check_tenant: true }\n  - guard: ContentModerationGuard\nresponse_guards:\n  - guard: PHIDisclosureGuard\n  - guard: DeAnonymizationGuard\n  - guard: HallucinationDetectionGuard\n    config: { threshold: 0.7 }\nengines:\n  consent:\n    store: postgres\n    verification_methods: [\"knowledge_based\"]\n  cost:\n    pricing_version: \"2024-12\"\n    daily_budget: 100\n    monthly_budget: 2000\n  audit:\n    retention_days: 365\n    export_format: json\n</code></pre>"},{"location":"architecture/agentic-governance-safety-framework/#storage-registration-layer","title":"Storage &amp; Registration Layer","text":"<p>\ud83d\udcd6 Storage Split: Usage/cost data lives in the LLM Gateway's database. Governance maintains separate stores for compliance-specific data.</p> <p>Governance-Specific Stores (not in gateway):</p> <p>1. Consent Store</p> <pre><code>interface ConsentStore {\n  save(consent: ConsentRecord): Promise&lt;void&gt;;\n  getActiveConsent(userId, consentType): Promise&lt;ConsentRecord | null&gt;;\n  getConsentHistory(userId): Promise&lt;ConsentRecord[]&gt;;\n  update(consent: ConsentRecord): Promise&lt;void&gt;;\n}\n</code></pre> <ul> <li>Implementation: PostgreSQL <code>consent_records</code> table with encryption at rest</li> <li>Retention: Per domain profile configuration (e.g., healthcare: 6 years, education: 5 years)</li> </ul> <p>2. Governance Audit Store (not LLM call logs)</p> <pre><code>interface GovernanceAuditStore {\n  logGovernanceEvent(event: GovernanceAuditEvent): Promise&lt;void&gt;;\n  getConsentEvents(tenantId, userId?, dateRange): Promise&lt;ConsentEvent[]&gt;;\n  getGuardDecisions(tenantId, dateRange): Promise&lt;GuardDecision[]&gt;;\n  exportComplianceReport(tenantId, regulation, dateRange): Promise&lt;ExportResult&gt;;\n}\n</code></pre> <ul> <li>Implementation: PostgreSQL <code>governance_audit_events</code> table</li> <li>Retention: Per domain profile configuration</li> <li>Note: LLM request/response logs are in the gateway's <code>LiteLLM_SpendLogs</code></li> </ul> <p>3. Anonymization Mapping Store</p> <pre><code>interface AnonymizationStore {\n  saveMapping(requestId, mapping): Promise&lt;string&gt;;\n  getMapping(mappingId): Promise&lt;AnonymizationMapping | null&gt;;\n  expireMapping(mappingId): Promise&lt;void&gt;;\n}\n</code></pre> <ul> <li>Implementation: PostgreSQL <code>anonymization_mappings</code> table</li> <li>Short-lived: Mappings expire after response delivery</li> </ul> <p>4. Policy Store</p> <pre><code>interface PolicyStore {\n  getDomainProfile(profileName): Promise&lt;DomainProfile&gt;;\n  getGovernancePipeline(pipelineName): Promise&lt;GovernancePipeline&gt;;\n  getTenantConfig(tenantId): Promise&lt;TenantConfig&gt;;\n}\n</code></pre> <ul> <li>Implementation: YAML files in git (versioned) + runtime cache</li> <li>Tenant overrides stored in PostgreSQL</li> </ul> <p>Gateway Data Access (query, don't duplicate):</p> <pre><code>interface GatewayDataAccess {\n  // Query LiteLLM's spend tracking\n  getTenantSpend(tenantId, dateRange): Promise&lt;SpendSummary&gt;;\n  getUserSpend(userId, tenantId, dateRange): Promise&lt;SpendSummary&gt;;\n\n  // Query LiteLLM's request logs for compliance reports\n  getRequestLogs(tenantId, dateRange, filters?): Promise&lt;RequestLog[]&gt;;\n}\n</code></pre> <ul> <li>Implementation: Read-only queries against LiteLLM's PostgreSQL tables</li> <li>Used for: Compliance cost reports, usage attribution</li> </ul>"},{"location":"architecture/agentic-governance-safety-framework/#multi-tenant-support","title":"Multi-Tenant Support","text":"<p>Every governance operation is scoped to a tenant:</p> <ul> <li><code>tenant.slug</code> determines storage partitions (consent, usage, audit tables)</li> <li>Tenant-specific budget limits and policy overrides</li> <li>Tenant-specific consent configurations</li> </ul> <p>Tenant\u2013Profile Relationship:</p> <p>A single tenant can operate across multiple domain profiles. For example, a healthcare education company might use: - <code>health_byoeb</code> profile for patient-facing content - <code>education_shiksha</code> profile for provider training content</p> <p>The governance context carries both <code>tenant_id</code> and <code>domain_profile</code>, allowing: - Shared billing and usage tracking across profiles - Profile-specific consent and compliance requirements - Unified audit trail across all interactions</p>"},{"location":"architecture/agentic-governance-safety-framework/#how-this-hooks-back-into-the-agentic-layer","title":"How This Hooks Back into the Agentic Layer","text":"<p>\ud83d\udcd6 Gateway Internals: For detailed Gateway architecture and internals, see LLM Gateway Layer Architecture.</p> <p>From the Agent Orchestrator's point of view, Governance is a wrapper that layers on top of the LLM Gateway.</p> <p>Governance-Specific Tools (not provided by gateway):</p> <ul> <li> <p>Tool: <code>check_consent</code></p> <ul> <li>Inputs: { <code>user_id</code>, <code>consent_type</code>, <code>data_category</code> }</li> <li>Output: { <code>has_consent</code>, <code>reason</code> }</li> <li>Why governance-specific: Domain-specific consent tracking (parental, data processing authorization)</li> </ul> </li> <li> <p>Tool: <code>check_domain_compliance</code></p> <ul> <li>Inputs: { <code>text</code>, <code>domain_profile</code> }</li> <li>Output: { <code>compliant</code>, <code>violations[]</code> }</li> <li>Why governance-specific: Domain-aware rules (PHI handling, age-appropriate content)</li> </ul> </li> <li> <p>Tool: <code>anonymize_with_mapping</code></p> <ul> <li>Inputs: { <code>text</code>, <code>domain_profile</code> }</li> <li>Output: { <code>anonymized_text</code>, <code>mapping</code> }</li> <li>Why governance-specific: Reversible anonymization for response restoration</li> </ul> </li> <li> <p>Tool: <code>log_governance_event</code></p> <ul> <li>Inputs: { <code>event_type</code>, <code>context</code>, <code>details</code> }</li> <li>Output: { <code>event_id</code> }</li> <li>Why governance-specific: Compliance audit trail (consent events, guard decisions)</li> </ul> </li> </ul> <p>This gives one Governance interface for all three systems &amp; any new ones, while leveraging the LLM Gateway Layer for foundational LLM operations.</p>"},{"location":"architecture/agentic-governance-safety-framework/#backward-compatibility-versioning-strategy","title":"Backward Compatibility &amp; Versioning Strategy","text":""},{"location":"architecture/agentic-governance-safety-framework/#register-current-safety-mechanisms-as-v0x-profiles","title":"Register Current Safety Mechanisms as v0.x Profiles","text":"<p>BYOEB - Current prompt engineering and content validation becomes profile <code>byoeb_health</code> version <code>0.1</code> - MD5 hashing of user IDs continues; governance adds PHI detection layer on top - Existing rate limiting becomes part of <code>BudgetGuard</code></p> <p>Shiksha Copilot - Current topic restrictions in prompts become <code>TopicRestrictionGuard</code> - Existing Azure auth becomes identity provider for consent checks - Token tracking in logs becomes formal cost tracking</p> <p>SEEDS - Currently AI-free; governance prepares for future AI features - Phone number handling gets PII detection - Rate limiting becomes formal budget control</p>"},{"location":"architecture/agentic-governance-safety-framework/#introducing-v1x-profiles","title":"Introducing v1.x Profiles","text":"<p>For each domain we can then evolve:</p> <p>byoeb_health v1.0: Adds healthcare compliance layer.</p> <p>Guards: - <code>DataProcessingAuthorizationGuard</code> \u2014 Check authorization before PHI access - <code>PHIFilterGuard</code> \u2014 Detect and anonymize healthcare identifiers - <code>PHIDisclosureGuard</code> \u2014 Prevent unauthorized PHI in responses</p> <p>Engines: - <code>ConsentEngine</code> with data processing authorization tracking - <code>AuditEngine</code> with configurable retention and breach detection</p> <p>shiksha_education v1.0: Adds education compliance layer.</p> <p>Guards: - <code>ParentalConsentGuard</code> \u2014 Verify parental consent for minors - <code>AgeVerificationGuard</code> \u2014 Age-gate before collecting data - <code>DisclosureConsentGuard</code> \u2014 Track education record disclosures</p> <p>Engines: - <code>ConsentEngine</code> with parental verification workflow - <code>AuditEngine</code> with disclosure logging</p> <p>seeds_accessibility v1.0: Extends education profile for future AI features.</p> <p>Guards: - Inherits education guards - <code>VoiceConsentGuard</code> \u2014 Verbal consent for IVR interactions</p> <p>Engines: - <code>ConsentEngine</code> adapted for voice channel - <code>AuditEngine</code> with call metadata</p> <p>generic_governance v1.0: A starter profile for new applications.</p> <pre><code>governance_pipeline: generic_v1\ndomain_profile: generic\nrequest_guards:\n  - guard: PIIDetectionGuard\n    config: { pii_types: [\"name\", \"email\", \"phone\"] }\n  - guard: BudgetGuard\n    config: { daily_limit: 100, monthly_limit: 2000 }\n  - guard: ContentModerationGuard\nresponse_guards:\n  - guard: ContentModerationGuard\nengines:\n  cost:\n    daily_budget: 100\n    monthly_budget: 2000\n  audit:\n    retention_days: 90\n</code></pre> <p>New applications can: 1. Start with generic_v1 to get basic governance immediately 2. Fork and customize \u2014 add domain-specific guards, adjust policies 3. Graduate to a named profile \u2014 register as <code>myapp_v1</code> with specific requirements</p>"},{"location":"architecture/agentic-governance-safety-framework/#low-level-design","title":"Low Level Design","text":""},{"location":"architecture/agentic-governance-safety-framework/#database-architecture","title":"Database Architecture","text":"<p>\u26a0\ufe0f Gateway vs Governance Storage: The LLM Gateway Layer (LiteLLM) maintains its own PostgreSQL database for spend tracking, usage records, and request logging. The governance framework maintains separate tables for compliance-specific data that the gateway doesn't track.</p>"},{"location":"architecture/agentic-governance-safety-framework/#what-the-gateway-already-stores","title":"What the Gateway Already Stores","text":"<p>\ud83d\udcd6 Gateway Logging Details: For what data the Gateway logs and how to configure observability, see LLM Gateway Layer Audit Logging.</p> <p>The LLM Gateway (LiteLLM) provides these tables out-of-the-box \u2014 do not duplicate:</p> LiteLLM Table Purpose Governance Access <code>LiteLLM_SpendLogs</code> Per-request cost, tokens, model, latency Query for compliance reports <code>LiteLLM_UserTable</code> User spend totals, budgets Query for budget status <code>LiteLLM_TeamTable</code> Team/tenant spend, limits Map to governance tenants <code>LiteLLM_VerificationToken</code> Virtual keys with budgets Manage via LiteLLM API <p>Integration Pattern: Query LiteLLM's database for cost/usage data; don't maintain separate usage tables.</p>"},{"location":"architecture/agentic-governance-safety-framework/#governance-specific-tables","title":"Governance-Specific Tables","text":"<p>These tables store data the gateway cannot provide:</p> <p>tenants (shared with Ingestion Framework) <pre><code>CREATE TABLE tenants (\n  id            UUID PRIMARY KEY,\n  slug          TEXT UNIQUE NOT NULL,\n  display_name  TEXT NOT NULL,\n  litellm_team_id TEXT,                       -- Maps to LiteLLM team for spend queries\n  created_at    TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>consent_records \u2014 regulatory consent tracking (not in gateway) <pre><code>CREATE TABLE consent_records (\n  id                  UUID PRIMARY KEY,\n  tenant_id           UUID REFERENCES tenants(id),\n  user_id             TEXT NOT NULL,\n  consent_type        TEXT NOT NULL,           -- 'PARENTAL_CONSENT', 'DATA_PROCESSING_AUTHORIZATION', 'DISCLOSURE_CONSENT'\n  status              TEXT NOT NULL,           -- 'pending', 'granted', 'denied', 'withdrawn', 'expired'\n  data_categories     JSONB NOT NULL,          -- ['ai_processing', 'phi_access']\n  processing_purposes JSONB NOT NULL,\n  third_parties       JSONB,\n  requested_at        TIMESTAMPTZ NOT NULL DEFAULT now(),\n  granted_at          TIMESTAMPTZ,\n  expires_at          TIMESTAMPTZ,\n  withdrawn_at        TIMESTAMPTZ,\n  verification_method TEXT,                    -- 'credit_card', 'knowledge_based', 'video_call'\n  verified_by         TEXT,                    -- parent/guardian user ID for parental consent\n  ip_address          INET,\n  user_agent          TEXT,\n  UNIQUE(tenant_id, user_id, consent_type)\n);\n\nCREATE INDEX idx_consent_user ON consent_records(tenant_id, user_id, status);\n</code></pre></p> <p>governance_audit_events \u2014 compliance-specific events (not LLM call logs) <pre><code>-- NOTE: LLM request/response logging is in LiteLLM's SpendLogs table.\n-- This table is ONLY for governance-specific events the gateway doesn't track.\n\nCREATE TABLE governance_audit_events (\n  id              UUID PRIMARY KEY,\n  tenant_id       UUID REFERENCES tenants(id),\n  timestamp       TIMESTAMPTZ NOT NULL DEFAULT now(),\n  event_type      TEXT NOT NULL,              -- Governance events only (see below)\n  request_id      UUID,                       -- Links to LiteLLM SpendLogs if applicable\n  user_id         TEXT,\n\n  -- Consent events\n  consent_type    TEXT,\n  consent_status  TEXT,\n\n  -- Guard decision events  \n  guard_name      TEXT,\n  guard_decision  TEXT,                       -- 'allow', 'block', 'modify', 'escalate'\n  guard_reason    TEXT,\n  violations      JSONB,\n\n  -- PII/PHI events (domain-specific detection)\n  pii_types_detected JSONB,                   -- ['mrn', 'health_condition', 'student_id']\n  anonymization_mapping_id UUID,              -- Reference to stored mapping for deanonymization\n\n  -- Metadata\n  domain_profile  TEXT,\n  ip_address      INET,\n  user_agent      TEXT,\n  metadata        JSONB\n);\n\nCREATE INDEX idx_gov_audit_tenant_date ON governance_audit_events(tenant_id, timestamp);\nCREATE INDEX idx_gov_audit_request ON governance_audit_events(request_id);\nCREATE INDEX idx_gov_audit_type ON governance_audit_events(tenant_id, event_type, timestamp);\n</code></pre></p> <p>Governance Event Types (what this table tracks): - <code>CONSENT_REQUESTED</code>, <code>CONSENT_GRANTED</code>, <code>CONSENT_DENIED</code>, <code>CONSENT_WITHDRAWN</code> - <code>GUARD_BLOCK</code>, <code>GUARD_MODIFY</code>, <code>GUARD_ESCALATE</code> - <code>DOMAIN_PII_DETECTED</code> (PHI, education records \u2014 beyond gateway's generic PII) - <code>ANONYMIZATION_APPLIED</code>, <code>DEANONYMIZATION_APPLIED</code> - <code>COMPLIANCE_VIOLATION</code>, <code>ESCALATION_CREATED</code></p> <p>anonymization_mappings \u2014 reversible anonymization storage (not in gateway) <pre><code>CREATE TABLE anonymization_mappings (\n  id              UUID PRIMARY KEY,\n  tenant_id       UUID REFERENCES tenants(id),\n  request_id      UUID NOT NULL,\n  mapping         JSONB NOT NULL,             -- {'[REDACTED_MRN_1]': 'MRN123456', ...}\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now(),\n  expires_at      TIMESTAMPTZ,                -- Auto-cleanup after response delivered\n  used_at         TIMESTAMPTZ                 -- When deanonymization was applied\n);\n\nCREATE INDEX idx_anon_request ON anonymization_mappings(request_id);\n</code></pre></p> <p>governance_pipelines \u2014 versioned governance configurations (not in gateway) <pre><code>CREATE TABLE governance_pipelines (\n  id              UUID PRIMARY KEY,\n  name            TEXT NOT NULL,              -- 'byoeb_health_v1'\n  domain_profile  TEXT NOT NULL,\n  version         TEXT NOT NULL,\n  config_yaml     TEXT NOT NULL,              -- Full pipeline config\n  is_default      BOOLEAN NOT NULL DEFAULT false,\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now(),\n  UNIQUE(name, version)\n);\n</code></pre></p> <p>governance_alert_config \u2014 alert thresholds beyond gateway's hard limits (governance extension) <pre><code>-- NOTE: Budget enforcement (blocking) is in LiteLLM via virtual key limits.\n-- This table configures warning alerts BEFORE the gateway blocks.\n\nCREATE TABLE governance_alert_config (\n  tenant_id                 UUID REFERENCES tenants(id) PRIMARY KEY,\n  daily_warning_threshold   INTEGER NOT NULL DEFAULT 80,   -- % of LiteLLM budget\n  monthly_warning_threshold INTEGER NOT NULL DEFAULT 80,\n  alert_channels            JSONB,                         -- ['email', 'slack', 'webhook']\n  alert_recipients          JSONB,\n  updated_at                TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p>"},{"location":"architecture/agentic-governance-safety-framework/#guard-engine-abstractions","title":"Guard &amp; Engine Abstractions","text":"<p>Guard Interface</p> <p>All guards implement a common protocol:</p> <pre><code>class Guard(Protocol):\n    name: str\n\n    async def evaluate(self, context: GovernanceContext) -&gt; GuardResult:\n        ...\n</code></pre> <p>Where: <pre><code>@dataclass\nclass GuardResult:\n    decision: GuardDecision  # ALLOW, BLOCK, MODIFY, ESCALATE\n    reason: str | None = None\n    modified_content: str | None = None\n    violations: list[str] | None = None\n    metadata: dict | None = None\n</code></pre></p> <p>Engine Interfaces</p> <p>\ud83d\udd11 Design Principle: Engines are categorized by their relationship to the Gateway: - Governance-Only: No Gateway equivalent (Consent) - Gateway-Extending: Adds domain-specific logic on top of Gateway (Safety) - Gateway-Querying: Queries Gateway data, adds governance alerting/reporting (Cost, Audit)</p> <p>1. Consent Engine (Governance-Only \u2014 no Gateway equivalent)</p> <pre><code>class ConsentEngine(Protocol):\n    \"\"\"Regulatory consent tracking. Fully governance-specific.\"\"\"\n    # PROACTIVE: Check before processing\n    async def check(self, user_id: str, consent_type: str, category: str) -&gt; bool: ...\n\n    # REACTIVE: Manage consent lifecycle\n    async def request(self, user_id: str, consent_type: str, scope: dict) -&gt; str: ...\n    async def verify(self, consent_id: str, verification_data: dict) -&gt; bool: ...\n    async def withdraw(self, user_id: str, consent_type: str) -&gt; None: ...\n</code></pre> <p>2. Safety Engine (Gateway-Extending \u2014 adds domain-specific detection)</p> <pre><code>class GovernanceSafetyEngine(Protocol):\n    \"\"\"Domain-specific PII handling. Extends Gateway's generic Presidio.\"\"\"\n    # PROACTIVE: Detect and anonymize before LLM call\n    async def detect_domain_pii(self, text: str, profile: str) -&gt; list[PIIDetection]: ...\n    async def anonymize_with_mapping(self, text: str, detections: list) -&gt; tuple[str, str]: ...\n\n    # REACTIVE: Restore after LLM call\n    async def deanonymize(self, text: str, mapping_id: str) -&gt; str: ...\n</code></pre> <p>3. Cost Engine (Gateway-Querying \u2014 queries LiteLLM, adds alerting)</p> <pre><code>class GovernanceCostEngine(Protocol):\n    \"\"\"Cost alerting and reporting. Queries Gateway\u2014does NOT duplicate storage.\"\"\"\n    # PROACTIVE: Check budget before LLM call\n    async def check_budget_status(self, tenant_id: str) -&gt; BudgetStatus:\n        # Queries LiteLLM_TeamTable\n        ...\n    async def check_alert_threshold(self, tenant_id: str) -&gt; AlertStatus:\n        # Compares Gateway spend against governance_alert_config\n        ...\n\n    # REACTIVE: Generate reports from Gateway data\n    async def query_gateway_spend(self, tenant_id: str, date_range: DateRange) -&gt; SpendData: ...\n    async def generate_cost_report(self, tenant_id: str, regulation: str, date_range: DateRange) -&gt; CostReport: ...\n</code></pre> <p>4. Audit Engine (Gateway-Querying \u2014 logs governance events, queries Gateway for unified reports)</p> <pre><code>class GovernanceAuditEngine(Protocol):\n    \"\"\"Compliance audit trail. Logs governance events, links to Gateway logs.\"\"\"\n    # REACTIVE: Log governance-specific events\n    async def log_consent_event(self, event: ConsentEvent) -&gt; str: ...\n    async def log_guard_decision(self, context: GovernanceContext, result: GuardResult) -&gt; str: ...\n    async def log_pii_event(self, event: PIIEvent) -&gt; str: ...\n    async def link_to_gateway_log(self, governance_event_id: str, gateway_request_id: str) -&gt; None: ...\n\n    # ON-DEMAND: Unified compliance reporting\n    async def generate_compliance_report(self, tenant_id: str, regulation: str, date_range: DateRange) -&gt; ComplianceReport:\n        # Queries governance_audit_events + LiteLLM_SpendLogs\n        ...\n</code></pre>"},{"location":"architecture/agentic-governance-safety-framework/#governance-pipeline-runner","title":"Governance Pipeline Runner","text":"<p>The Governance Service wraps AI operations using configured pipelines:</p> <pre><code>class GovernanceRunner:\n    async def wrap(self, context: GovernanceContext, operation: Callable) -&gt; Any:\n        # 1. Run governance-specific request guards (consent, domain PII, topic)\n        for guard in self.pipeline.request_guards:\n            result = await guard.evaluate(context)\n            if result.decision == BLOCK:\n                await self.governance_audit.log_guard_decision(context, result)\n                raise GovernanceBlockedError(result.reason)\n            if result.decision == MODIFY:\n                context.content = result.modified_content\n\n        # 2. Check budget via gateway (LiteLLM handles enforcement)\n        #    Governance adds warning alerts before hard block\n        alert_status = await self.budget_checker.check_alert_threshold(context.tenant_id)\n        if alert_status.should_warn:\n            await self.send_budget_alert(context.tenant_id, alert_status)\n\n        # 3. Execute operation via LLM Gateway\n        #    Gateway handles: routing, failover, generic guardrails, cost tracking, logging\n        result = await operation(context)\n\n        # NOTE: Cost recording happens automatically in gateway's LiteLLM_SpendLogs\n        # No need to record here - governance queries gateway for reports\n\n        # 4. Run governance-specific response guards (PHI disclosure, minor data)\n        for guard in self.pipeline.response_guards:\n            guard_result = await guard.evaluate(context, result)\n            if guard_result.decision == BLOCK:\n                await self.governance_audit.log_guard_decision(context, guard_result)\n                raise GovernanceBlockedError(guard_result.reason)\n            if guard_result.decision == MODIFY:\n                result.content = guard_result.modified_content\n\n        # 5. Log governance-specific audit event (consent verified, guards passed)\n        #    NOTE: LLM request/response already logged by gateway\n        await self.governance_audit.log_request_completed(context)\n\n        return result\n</code></pre>"},{"location":"architecture/agentic-governance-safety-framework/#future-extensions","title":"Future Extensions","text":"<p>LLM Gateway Layer Integration - See LLM Gateway Layer Architecture for detailed implementation of the unified LLM access layer - Provides foundation for multi-provider strategy, cost optimization, and built-in guardrails - Recommended as the first foundational component to implement</p> <p>AI-Specific Regulatory Compliance (EU AI Act) - Risk classification for high-risk AI systems (healthcare, education) - Technical documentation generation for AI components - Bias testing and fairness monitoring - Explainability logging for AI decisions</p> <p>Advanced PII Detection - NER-based entity detection for context-aware PII - Multi-language PII detection (Hindi, Kannada for Indian deployments) - Audio PII detection for voice channels - Image/document PII detection for uploaded files</p> <p>Federated Governance - Cross-organization consent sharing - Distributed audit aggregation - Multi-jurisdiction compliance (US + EU)</p> <p>Human-in-the-Loop Workflows - Escalation routing to human reviewers - Expert consensus for edge cases (like BYOEB's expert loop) - Appeal and override mechanisms</p> <p>Real-Time Monitoring &amp; Alerting - Budget threshold alerts - Unusual activity detection - Compliance violation alerts - SLA monitoring for governance overhead</p> <p>MCP-Based Governance Tools - Expose governance as MCP tools for LLM agents - Enable agents to check consent, budget, and compliance status - Support autonomous compliance decisions within agent workflows</p>"},{"location":"architecture/llm-gateway-layer/","title":"LLM Gateway Layer: Cross-Cutting Infrastructure for AI Governance","text":""},{"location":"architecture/llm-gateway-layer/#executive-summary","title":"Executive Summary","text":"<p>The LLM Gateway Layer is a foundational infrastructure component within the Agentic Governance &amp; Safety Framework that provides unified, abstracted access to multiple Large Language Model (LLM) providers. This layer solves critical cross-cutting concerns around provider flexibility, cost optimization, failover resilience, and operational governance\u2014enabling organizations to swap, scale, and govern AI providers without application-level changes.</p> <p>This document evaluates the Build vs Buy decision, compares leading solutions (OpenRouter vs LiteLLM), and provides a recommended architecture integrating LiteLLM as the gateway layer within the Inclusive Architecture.</p>"},{"location":"architecture/llm-gateway-layer/#document-navigation","title":"Document Navigation","text":"<ul> <li>Understanding Governance? See Agentic Governance &amp; Safety Framework for domain-specific compliance</li> <li>Understanding integration? See Integration with Governance Framework (this document)</li> <li>Understanding responsibilities? See What Gateway Provides vs What Governance Adds</li> <li>Implementing Governance? See Governance Framework Engine Layer</li> </ul>"},{"location":"architecture/llm-gateway-layer/#problem-statement","title":"Problem Statement","text":""},{"location":"architecture/llm-gateway-layer/#current-challenges-across-products","title":"Current Challenges Across Products","text":"Product Current LLM Integration Pain Points BYOEB Direct Azure OpenAI SDK Vendor lock-in, no fallback, manual cost tracking Shiksha Copilot Azure OpenAI + Azure AI Projects Mixed SDK patterns, inconsistent token tracking SEEDS No active LLM (planned) Need flexible foundation for future AI features"},{"location":"architecture/llm-gateway-layer/#cross-cutting-concerns-requiring-gateway-solution","title":"Cross-Cutting Concerns Requiring Gateway Solution","text":"<ol> <li>Provider Lock-In: Direct SDK integrations create tight coupling to specific providers</li> <li>No Fallback Strategy: Service disruptions affect availability without automatic failover</li> <li>Manual Cost Tracking: Token counting requires custom implementation per provider</li> <li>Inconsistent API Patterns: Different SDKs require different code paths for same operations</li> <li>Limited Model Selection: Inability to leverage best-in-class models from different providers</li> <li>Compliance Gaps: No unified layer for governance policies across all LLM calls</li> </ol>"},{"location":"architecture/llm-gateway-layer/#build-vs-buy-analysis","title":"Build vs Buy Analysis","text":""},{"location":"architecture/llm-gateway-layer/#option-a-build-custom-gateway","title":"Option A: Build \u2014 Custom Gateway","text":"<p>Approach: Develop internal abstraction layer over multiple LLM provider SDKs.</p> Aspect Assessment Development Effort 3-6 months for MVP, ongoing maintenance Provider Coverage Limited to prioritized providers Feature Parity Continuous catch-up with provider updates Cost Tracking Requires manual pricing table maintenance Failover Logic Custom implementation needed Maintenance Burden High \u2014 API changes, new models, pricing updates <p>Hidden Costs: - Continuous SDK version updates - Pricing table maintenance (changes monthly) - New model support delays - Security patching across multiple SDKs - Documentation and onboarding</p> <p>Verdict: \u274c Not Recommended \u2014 High ongoing cost, diverts resources from core product development.</p>"},{"location":"architecture/llm-gateway-layer/#option-b-buy-llm-gateway-solutions","title":"Option B: Buy \u2014 LLM Gateway Solutions","text":"<p>Two mature gateway solutions exist that provide unified LLM access: OpenRouter (managed SaaS) and LiteLLM (self-hosted open source).</p>"},{"location":"architecture/llm-gateway-layer/#feature-comparison","title":"Feature Comparison","text":"Capability OpenRouter LiteLLM Winner Deployment Model \u2601\ufe0f Managed SaaS \ud83c\udfe0 Self-hosted (Docker/K8s) Depends on needs Integration OpenAI-compatible API OpenAI-compatible API \ud83e\udd1d Tie Provider Coverage 100+ models 100+ providers \ud83e\udd1d Tie BYOK Support \u2705 Yes \u2705 Yes \ud83e\udd1d Tie Cost Tracking Dashboard only \u2705 Per-request + database LiteLLM Budget Controls Basic limits \u2705 Per-key/user/team LiteLLM Failover/Routing Automatic \u2705 Configurable strategies LiteLLM Rate Limiting Basic \u2705 Granular (RPM/TPM) LiteLLM Guardrails \u274c None \u2705 Presidio, Lakera, etc. LiteLLM Data Residency \u26a0\ufe0f US-based servers \u2705 Your infrastructure LiteLLM Compliance Ready \u26a0\ufe0f Third-party handling \u2705 No data leaves infra LiteLLM Pricing Pass-through + margin Free (open source) LiteLLM Infrastructure Overhead \u2705 Zero Requires deployment OpenRouter New Model Availability \u2705 Instant Manual config update OpenRouter"},{"location":"architecture/llm-gateway-layer/#when-to-choose-each","title":"When to Choose Each","text":"Scenario Recommendation Rapid prototyping / POC OpenRouter \u2014 zero setup Non-sensitive data workloads OpenRouter \u2014 simpler Regulatory compliance (healthcare, education) LiteLLM \u2014 data residency, no third-party exposure Enterprise governance requirements LiteLLM \u2014 budget controls, guardrails Multi-tenant SaaS with billing LiteLLM \u2014 per-team cost tracking Cost optimization at scale LiteLLM \u2014 no margin, routing strategies"},{"location":"architecture/llm-gateway-layer/#verdict","title":"Verdict","text":"<p>LiteLLM is the recommended choice because: 1. Data residency \u2014 Critical for compliance needs 2. Built-in governance \u2014 Cost tracking, budgets, guardrails reduce custom implementation 3. No cost margin \u2014 Direct provider pricing at scale 4. Extensibility \u2014 Custom guardrails for PHI, domain-specific compliance</p>"},{"location":"architecture/llm-gateway-layer/#detailed-litellm-evaluation","title":"Detailed LiteLLM Evaluation","text":""},{"location":"architecture/llm-gateway-layer/#provider-support-matrix","title":"Provider Support Matrix","text":"<p>LiteLLM supports 100+ providers via a unified OpenAI-compatible API. Feature support for key providers:</p> Provider Model Prefix Key Models Vision Tools Streaming Embeddings Azure OpenAI <code>azure/&lt;deployment&gt;</code> GPT-4o, GPT-4, GPT-3.5 \u2705 \u2705 \u2705 \u2705 OpenAI <code>openai/</code> GPT-4o, o1, o3-mini \u2705 \u2705 \u2705 \u2705 Anthropic <code>anthropic/</code> Claude 3.5 Sonnet, Opus, Haiku \u2705 \u2705 \u2705 \u274c Google Gemini <code>gemini/</code> Gemini 1.5 Pro/Flash, 2.0 \u2705 \u2705 \u2705 \u2705 xAI <code>xai/</code> Grok-2, Grok-beta \u2705 \u2705 \u2705 \u274c Meta Llama <code>together_ai/</code> Llama 3.1, 3.2 (via hosts) \u2705 \u2705 \u2705 \u274c Mistral <code>mistral/</code> Large, Medium, Codestral \u2705 \u2705 \u2705 \u2705 AWS Bedrock <code>bedrock/</code> Claude, Titan, Llama \u2705 \u2705 \u2705 \u2705 Cohere <code>cohere/</code> Command R, R+ \u274c \u2705 \u2705 \u2705 Groq <code>groq/</code> Llama, Mixtral (fast) \u274c \u2705 \u2705 \u274c <p>\ud83d\udcdd Note: Llama models require a hosting provider (Together AI, Groq, AWS Bedrock, or self-hosted via Ollama).</p> <p>Usage: <code>response = completion(model=\"&lt;prefix&gt;/&lt;model&gt;\", messages=[...])</code></p>"},{"location":"architecture/llm-gateway-layer/#built-in-governance-features","title":"Built-in Governance Features","text":"<p>LiteLLM provides native governance capabilities that align directly with our Agentic Governance &amp; Safety Framework:</p>"},{"location":"architecture/llm-gateway-layer/#1-cost-tracking-budget-management","title":"1. Cost Tracking &amp; Budget Management \u2705","text":"<p>Per-Request Cost Calculation: <pre><code>response = completion(model=\"gpt-4o\", messages=messages)\n\n# Automatic cost calculation\nprint(response._hidden_params[\"response_cost\"])  # e.g., 0.0035\n</code></pre></p> <p>Budget Enforcement: <pre><code># config.yaml\ngeneral_settings:\n  master_key: \"sk-xxx\"\n  database_url: \"postgresql://...\"\n\nlitellm_settings:\n  max_budget: 1000  # Monthly budget in USD\n  budget_duration: \"monthly\"\n\nmodel_list:\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o-deployment\n      api_key: os.environ/AZURE_API_KEY\n    model_info:\n      max_budget: 100  # Per-model budget\n</code></pre></p> <p>Per-User/Per-Team Budgets (via Virtual Keys): <pre><code># Create user with budget\ncurl -X POST 'http://proxy:4000/user/new' \\\n  -H 'Authorization: Bearer sk-master-key' \\\n  -d '{\n    \"user_id\": \"user-123\",\n    \"max_budget\": 50,\n    \"budget_duration\": \"daily\"\n  }'\n</code></pre></p> <p>Governance Framework Alignment: Maps directly to our <code>CostEngine</code> requirements. For how Governance extends these capabilities with alert thresholds and compliance reporting, see Governance Cost Engine.</p>"},{"location":"architecture/llm-gateway-layer/#2-guardrails-content-safety","title":"2. Guardrails &amp; Content Safety \u2705","text":"<p>Native Guardrail Integration: <pre><code># config.yaml\nlitellm_settings:\n  guardrails:\n    - guardrail_name: \"aporia-guard\"\n      litellm_params:\n        guardrail: aporia\n        api_key: os.environ/APORIA_API_KEY\n        api_base: \"https://api.aporia.com\"\n\n    - guardrail_name: \"lakera-pii-guard\"  \n      litellm_params:\n        guardrail: lakera\n        api_key: os.environ/LAKERA_API_KEY\n</code></pre></p> <p>Supported Guardrail Providers: | Provider | Capabilities | |----------|--------------| | Aporia | PII detection, prompt injection, topic filtering | | Lakera | Prompt injection, jailbreak detection, PII | | Bedrock Guardrails | AWS-native content filtering | | Azure AI Content Safety | Microsoft content moderation | | Presidio | Open-source PII detection | | LlamaGuard | Meta's safety classifier | | Google Text Moderation | Harmful content detection | | OpenAI Moderation | Built-in moderation endpoint |</p> <p>Custom Guardrails: <pre><code>from litellm.proxy.guardrails.guardrail_hooks import GuardrailItem\n\nclass CustomPHIGuard(GuardrailItem):\n    async def async_pre_call_hook(self, data, user_api_key_dict, call_type):\n        # Implement PHI detection logic\n        if self.contains_phi(data[\"messages\"]):\n            raise GuardrailException(\"PHI detected in request\")\n        return data\n</code></pre></p> <p>Governance Framework Alignment: Maps to our <code>SafetyEngine</code> and Guard Layer requirements. For how Governance extends these capabilities with domain-specific PII detection (PHI, education records) and reversible anonymization, see Governance Safety Engine.</p>"},{"location":"architecture/llm-gateway-layer/#3-authentication-access-control","title":"3. Authentication &amp; Access Control \u2705","text":"<p>Virtual Keys (API Key Management): <pre><code># Generate scoped API key\ncurl -X POST 'http://proxy:4000/key/generate' \\\n  -H 'Authorization: Bearer sk-master-key' \\\n  -d '{\n    \"duration\": \"30d\",\n    \"models\": [\"gpt-4o\", \"claude-sonnet-4-20250514\"],\n    \"max_budget\": 100,\n    \"metadata\": {\"team\": \"byoeb\", \"environment\": \"production\"}\n  }'\n</code></pre></p> <p>JWT Authentication: <pre><code>general_settings:\n  master_key: \"sk-xxx\"\n  litellm_jwtauth:\n    user_id_jwt_field: \"sub\"\n    team_id_jwt_field: \"org_id\"\n    user_email_jwt_field: \"email\"\n</code></pre></p> <p>SSO Integration (Enterprise): - OIDC/OAuth2 support - Azure AD, Google Workspace, Okta - SAML for enterprise SSO</p> <p>Governance Framework Alignment: Supports our multi-tenant authentication requirements.</p>"},{"location":"architecture/llm-gateway-layer/#4-audit-logging-observability","title":"4. Audit Logging &amp; Observability \u2705","text":"<p>Comprehensive Logging: <pre><code>litellm_settings:\n  callbacks:\n    - \"langfuse\"        # LLM observability platform\n    - \"lunary\"          # Open-source LLM monitoring  \n    - \"logfire\"         # Pydantic's observability\n    - \"prometheus\"      # Metrics export\n    - \"otel\"            # OpenTelemetry\n</code></pre></p> <p>Database Logging (Spend tracking): <pre><code>general_settings:\n  database_url: \"postgresql://user:pass@host:5432/litellm\"\n  store_model_in_db: true\n</code></pre></p> <p>Logged Data: - Request/response metadata - Token counts (input, output, cached) - Cost per request - Latency metrics - User/team attribution - Model used - Error details</p> <p>\ud83d\udcd6 Governance Access: For how Governance accesses this data and extends it with compliance-specific audit trails, see Governance Database Architecture.</p> <p>Governance Framework Alignment: Maps directly to our <code>AuditEngine</code> requirements. For how Governance extends these capabilities with compliance-specific audit trails, consent events, and regulatory reporting, see Governance Audit Engine.</p>"},{"location":"architecture/llm-gateway-layer/#5-fallback-reliability","title":"5. Fallback &amp; Reliability \u2705","text":"<p>Automatic Failover: <pre><code>model_list:\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o-eastus\n      api_base: \"https://eastus.openai.azure.com\"\n    model_info:\n      id: \"azure-eastus-primary\"\n\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o-westus\n      api_base: \"https://westus.openai.azure.com\"\n    model_info:\n      id: \"azure-westus-fallback\"\n\nrouter_settings:\n  routing_strategy: \"latency-based-routing\"\n  fallbacks: [{\"gpt-4o\": [\"claude-sonnet-4-20250514\"]}]\n  allowed_fails: 3\n  cooldown_time: 60\n</code></pre></p> <p>Routing Strategies: - <code>simple-shuffle</code>: Random distribution - <code>least-busy</code>: Route to model with lowest active requests - <code>latency-based-routing</code>: Route to fastest responding model - <code>cost-based-routing</code>: Route to cheapest available model</p> <p>Governance Framework Alignment: Provides resilience for mission-critical healthcare/education applications.</p>"},{"location":"architecture/llm-gateway-layer/#6-rate-limiting","title":"6. Rate Limiting \u2705","text":"<p>Per-Key Rate Limits: <pre><code>litellm_settings:\n  default_team_settings:\n    rpm_limit: 100      # Requests per minute\n    tpm_limit: 100000   # Tokens per minute\n</code></pre></p> <p>Per-Model Rate Limits: <pre><code>model_list:\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o\n    model_info:\n      rpm_limit: 50\n      tpm_limit: 50000\n</code></pre></p> <p>Governance Framework Alignment: Maps to our <code>RateLimitGuard</code> requirements.</p>"},{"location":"architecture/llm-gateway-layer/#enterprise-features-optional","title":"Enterprise Features (Optional)","text":"<p>For organizations requiring additional governance, LiteLLM Enterprise provides:</p> Feature Description SSO for Admin UI OIDC/SAML authentication IP Access Control Whitelist/blacklist IP ranges PII Masking Automatic PII redaction in logs Custom Branding White-label admin interface Priority Support Dedicated support channel SLA Guarantees Uptime and response time SLAs <p>Note: Open source version covers all core governance requirements.</p>"},{"location":"architecture/llm-gateway-layer/#recommended-architecture","title":"Recommended Architecture","text":""},{"location":"architecture/llm-gateway-layer/#integration-with-agentic-governance-safety-framework","title":"Integration with Agentic Governance &amp; Safety Framework","text":"<p>\ud83d\udcd6 Governance Layer Details: For Governance-specific architecture and how it wraps the Gateway, see Governance Framework Architecture.</p>"},{"location":"architecture/llm-gateway-layer/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code># docker-compose.yml for LiteLLM Gateway\nversion: \"3.9\"\n\nservices:\n  litellm-proxy:\n    image: ghcr.io/berriai/litellm:main-latest\n    ports:\n      - \"4000:4000\"\n    environment:\n      - DATABASE_URL=postgresql://litellm:password@postgres:5432/litellm\n      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}\n      - AZURE_API_KEY=${AZURE_API_KEY}\n      - AZURE_API_BASE=${AZURE_API_BASE}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n      - GOOGLE_API_KEY=${GOOGLE_API_KEY}\n    volumes:\n      - ./config.yaml:/app/config.yaml\n    command: [\"--config\", \"/app/config.yaml\", \"--detailed_debug\"]\n    depends_on:\n      - postgres\n      - redis\n\n  postgres:\n    image: postgres:15\n    environment:\n      - POSTGRES_USER=litellm\n      - POSTGRES_PASSWORD=password\n      - POSTGRES_DB=litellm\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  postgres_data:\n  redis_data:\n</code></pre>"},{"location":"architecture/llm-gateway-layer/#configuration-for-multi-tenant-governance","title":"Configuration for Multi-Tenant Governance","text":"<pre><code># litellm_config.yaml\nmodel_list:\n  # Azure OpenAI - Primary for BYOEB (Healthcare)\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o-deployment\n      api_base: os.environ/AZURE_API_BASE\n      api_key: os.environ/AZURE_API_KEY\n      api_version: \"2024-02-15-preview\"\n    model_info:\n      id: \"azure-gpt4o-primary\"\n      max_budget: 500\n      rpm_limit: 100\n      tpm_limit: 100000\n\n  # Azure OpenAI - Fallback\n  - model_name: gpt-4o\n    litellm_params:\n      model: azure/gpt-4o-westus\n      api_base: os.environ/AZURE_WESTUS_API_BASE\n      api_key: os.environ/AZURE_WESTUS_API_KEY\n    model_info:\n      id: \"azure-gpt4o-fallback\"\n\n  # Anthropic Claude - Alternative for reasoning tasks\n  - model_name: claude-sonnet\n    litellm_params:\n      model: claude-sonnet-4-20250514\n      api_key: os.environ/ANTHROPIC_API_KEY\n    model_info:\n      max_budget: 200\n\n  # Google Gemini - Cost-effective option\n  - model_name: gemini-flash\n    litellm_params:\n      model: gemini/gemini-2.0-flash\n      api_key: os.environ/GOOGLE_API_KEY\n    model_info:\n      max_budget: 100\n\n  # Embeddings - Azure\n  - model_name: text-embedding-3-large\n    litellm_params:\n      model: azure/text-embedding-3-large\n      api_base: os.environ/AZURE_API_BASE\n      api_key: os.environ/AZURE_API_KEY\n\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n  database_url: os.environ/DATABASE_URL\n  store_model_in_db: true\n\nrouter_settings:\n  routing_strategy: \"latency-based-routing\"\n  fallbacks: [{\"gpt-4o\": [\"claude-sonnet\", \"gemini-flash\"]}]\n  allowed_fails: 3\n  cooldown_time: 60\n  retry_after: 15\n\nlitellm_settings:\n  drop_params: true\n  max_budget: 2000\n  budget_duration: \"monthly\"\n\n  # Guardrails integration\n  guardrails:\n    - guardrail_name: \"presidio-pii\"\n      litellm_params:\n        guardrail: presidio\n\n    - guardrail_name: \"openai-moderation\"\n      litellm_params:\n        guardrail: openai_moderation\n\n  # Callbacks for observability\n  callbacks:\n    - \"langfuse\"\n    - \"prometheus\"\n\n  success_callback: [\"langfuse\"]\n  failure_callback: [\"langfuse\"]\n</code></pre>"},{"location":"architecture/llm-gateway-layer/#integration-with-governance-framework","title":"Integration with Governance Framework","text":"<p>The LLM Gateway Layer integrates with the Agentic Governance &amp; Safety Framework as the foundational infrastructure layer. The governance framework's engines interact with the Gateway as follows:</p> Governance Engine Gateway Interaction Cost Engine Queries <code>LiteLLM_TeamTable</code> for spend status; adds warning alerts before Gateway's hard budget block Safety Engine Adds domain-specific PII detection (PHI, education) on top of Gateway's generic Presidio/Lakera guardrails Audit Engine Logs governance-specific events (consent, guard decisions); links to Gateway logs via <code>gateway_request_id</code> <p>\ud83d\udcd6 Detailed Integration:  - Responsibility Boundaries: See What Gateway Provides vs What Governance Adds for complete capability matrix - Engine Interfaces: See Engine Layer section for complete interface definitions, proactive/reactive behavior, and implementation patterns - Integration Principles: Governance queries the Gateway\u2014it does NOT duplicate storage. All cost/usage data lives in LiteLLM's database tables.</p>"},{"location":"architecture/llm-gateway-layer/#migration-strategy","title":"Migration Strategy","text":""},{"location":"architecture/llm-gateway-layer/#phase-1-infrastructure-setup","title":"Phase 1: Infrastructure Setup","text":"<ol> <li>Deploy LiteLLM Proxy in staging environment</li> <li>Configure existing Azure OpenAI credentials</li> <li>Set up PostgreSQL for spend tracking</li> <li>Create tenant-specific virtual keys</li> </ol>"},{"location":"architecture/llm-gateway-layer/#phase-2-byoeb-migration","title":"Phase 2: BYOEB Migration","text":"<ol> <li>Update LLM client to point to LiteLLM proxy</li> <li>Map existing Azure OpenAI calls through gateway</li> <li>Enable cost tracking and validate accuracy</li> <li>Add Presidio guardrail for PII detection</li> <li>Monitor and tune fallback behavior</li> </ol>"},{"location":"architecture/llm-gateway-layer/#phase-3-shiksha-copilot-migration","title":"Phase 3: Shiksha Copilot Migration","text":"<ol> <li>Migrate Azure OpenAI calls through gateway</li> <li>Configure education-specific guardrails</li> <li>Enable Claude fallback for reasoning tasks</li> <li>Integrate with existing token tracking</li> </ol>"},{"location":"architecture/llm-gateway-layer/#phase-4-advanced-features","title":"Phase 4: Advanced Features","text":"<ol> <li>Add custom PHI guardrail for healthcare</li> <li>Implement cross-model routing based on task type</li> <li>Set up Langfuse/Prometheus observability</li> <li>Create governance dashboards</li> </ol>"},{"location":"architecture/llm-gateway-layer/#benefits-summary","title":"Benefits Summary","text":""},{"location":"architecture/llm-gateway-layer/#immediate-benefits","title":"Immediate Benefits","text":"Benefit Impact Provider Flexibility Switch/add providers without code changes Cost Visibility Real-time spend tracking per tenant/user Automatic Fallover 99.9% availability through provider redundancy Unified API Single SDK pattern across all products Built-in Safety PII detection, content moderation out-of-box"},{"location":"architecture/llm-gateway-layer/#long-term-strategic-benefits","title":"Long-term Strategic Benefits","text":"Benefit Impact Cost Optimization Route to cheapest capable model Best-of-Breed Models Use Claude for reasoning, GPT for general, Gemini for multimodal Compliance Foundation Audit trail, data residency, governance policies Future-Proofing New models available immediately Reduced Lock-in Negotiate better rates with optionality <p>LiteLLM is the recommended LLM Gateway solution for the Agentic Governance &amp; Safety Framework because it:</p> <ol> <li>\u2705 Supports all required providers (Azure OpenAI, Anthropic, Google, Meta, xAI, and 100+ more)</li> <li>\u2705 Provides native governance features (cost tracking, budgets, guardrails, audit logging)</li> <li>\u2705 Enables self-hosting for data residency and compliance (critical for healthcare/education domains)</li> <li>\u2705 Open source with optional enterprise features</li> <li>\u2705 Reduces implementation effort for our governance framework by providing foundational capabilities</li> </ol> <p>The LLM Gateway Layer should be implemented as the first foundational component of the governance framework, providing immediate value while enabling incremental addition of domain-specific governance logic.</p>"},{"location":"architecture/llm-gateway-layer/#references","title":"References","text":"<ul> <li>LiteLLM Documentation</li> <li>LiteLLM GitHub Repository</li> <li>OpenRouter Documentation</li> <li>Azure OpenAI Service</li> <li>Presidio PII Detection</li> <li>Langfuse LLM Observability</li> </ul>"},{"location":"architecture/universal-knowledge-ingestion/","title":"Universal Knowledge Ingestion Framework","text":""},{"location":"architecture/universal-knowledge-ingestion/#purpose-and-scope","title":"Purpose and Scope","text":"<p>This document proposes a common, inclusive knowledge ingestion framework that can be used for digital public AI good:</p> <ul> <li>BYOEB (ASHA WhatsApp RAG for healthcare)</li> <li>Shiksha Copilot (lesson outcome\u2013driven education copilot)</li> <li>SEEDS (audio-first IVR and conferencing for visually impaired learners)</li> </ul> <p>\u2026and is extensible to new products and modalities.</p> <p>It aligns with the Inclusive Architecture\u2019s Ingestion Layer and its agent/tool model \u2014 \u201cingestion trigger \u2192 retrieval \u2192 parsing \u2192 chunking \u2192 embedding \u2192 storage \u2192 logging\u201d \u2014 and implements those as reusable agents and tools across domains.</p>"},{"location":"architecture/universal-knowledge-ingestion/#layered-architecture","title":"Layered Architecture","text":""},{"location":"architecture/universal-knowledge-ingestion/#design-drivers","title":"Design drivers","text":"<ol> <li> <p>Aligned with Inclusive Architecture</p> <ul> <li>Ingestion is a layer serving multiple domains (education, health, accessibility) via a common set of agents &amp; tools.</li> <li>All products talk to ingestion through consistent APIs/events; ingestion itself is multi-tenant and domain-aware.</li> </ul> </li> <li> <p>Multi\u2011Modal</p> <ul> <li>Documents &amp; text: PDFs, DOCX, plain text, HTML, FAQs.</li> <li>Audio: IVR recordings, TTS outputs, podcasts/YouTube, etc.</li> <li>Future: images, structured data (CSV, logs), etc.</li> </ul> </li> <li> <p>Multi\u2011Model &amp; Multi\u2011Store</p> <ul> <li>Plug-and-play LLM providers (Azure OpenAI, OpenAI, OpenRouter, local models via Ollama).</li> <li>OpenRouter as a default unified gateway: Access 100+ models (Claude, Gemini, Llama, Mistral) through a single API, enabling cost optimization and model experimentation without provider lock-in.</li> <li>Pluggable vector stores (Azure Vector Search, Qdrant, Chroma, pgvector\u2026).</li> <li>Other stores: blob/file storage, relational/NoSQL metadata, graph DB (Neo4j), and search indexes.</li> </ul> </li> <li> <p>Domain-Aware but Reusable</p> <ul> <li>Shiksha's LO extraction and curriculum KG should be profiles on top of generic steps, not one-off pipelines.</li> <li>BYOEB's KB ingestion should be the default text ingestion profile.</li> <li>SEEDS' audio/transcription/knowledge items flow should be the default audio profile.</li> </ul> </li> <li> <p>Inclusive &amp; Operational</p> <ul> <li>Support low-resource environments (local vector stores, offline-first when possible).</li> <li>Strong observability, versioning, audit, and safety/quality checks.</li> </ul> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#conceptual-model","title":"Conceptual Model","text":"<p>We define three layers inside the ingestion system (itself mapped to the \u201cIngestion Layer\u201d in the Inclusive Architecture): </p> <ol> <li>Source &amp; Trigger Layer: Where content comes from and why we ingest it.</li> <li>Processing Pipeline Layer (Agents &amp; Tools): How content is parsed, cleaned, enriched, and encoded as knowledge.</li> <li>Storage &amp; Registration Layer: Where the knowledge representations live and how they are registered for use by RAG/agents.</li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#canonical-knowledge-item","title":"Canonical \"Knowledge Item\"","text":"<p>All pipelines ultimately produce a canonical <code>KnowledgeItem</code> (concept, not implementation-bound) with:</p> Field Description <code>id</code> UUID <code>tenant_id</code> Tenant/partner/org context <code>domain_profile</code> education_shiksha / health_byoeb / accessibility_seeds / etc. <code>modality</code> <code>text</code>, <code>document</code>, <code>audio</code>, <code>mixed</code> <code>source_uri</code> Blob URI / URL / IVR recording ID <code>metadata</code> Board/grade/subject/chapter, language, tags, timestamps <code>structured_views</code> LO trees, KG nodes, transcript segments <code>embeddings</code> Vector store pointers (no raw vectors in Postgres) <code>lineage</code> Pipeline version, model version, ingestion time <p>BYOEB\u2019s <code>TextNode</code> + vector metadata, Shiksha Copilot's <code>StepResult</code> and SEEDS\u2019 <code>KnowledgeItem</code> schema can be mapped into this conceptual model.</p>"},{"location":"architecture/universal-knowledge-ingestion/#high-level-ingestion-flow","title":"High-Level Ingestion Flow","text":"<p>We define a generic pipeline that all domain profiles reuse:</p> <ol> <li>Ingestion Trigger</li> <li>Data Retrieval</li> <li>Parsing &amp; Preprocessing</li> <li>Chunking &amp; Metadata Enrichment</li> <li>Embedding &amp; Indexing</li> <li>Storage &amp; Caching</li> <li>Event Logging &amp; Completion</li> </ol> <p>We implement each step as one or more agents (reasoning/LLM heavy) and tools (IO or deterministic work).</p>"},{"location":"architecture/universal-knowledge-ingestion/#core-implementation","title":"Core Implementation","text":""},{"location":"architecture/universal-knowledge-ingestion/#source-and-trigger-layer","title":"Source And Trigger Layer","text":"<p>Triggers:</p> <ul> <li>API calls (e.g., <code>/vector/index</code> in BYOEB)</li> <li>Webhook/events: \"file uploaded to blob\", \"new textbook version published\", \"audio recording completed from IVR\"</li> <li>Scheduled jobs: \"sync YouTube playlist weekly\", \"re-ingest content when embedding model changes\"</li> <li>Manual CLI/ops triggers</li> </ul> <p>Source Connectors:</p> <p>A standard source connector would implement the interface:</p> <pre><code>interface SourceConnector {\n  listResources(filter): Promise&lt;SourceResource[]&gt;;\n  fetch(resourceId): Promise&lt;RawArtifact&gt;;  // bytes + metadata\n}\n</code></pre> <p>Examples:</p> <ul> <li><code>AzureBlobConnector</code> \u2014 already exists in BYOEB.</li> <li><code>HttpUrlConnector</code> \u2014 general web pages / PDFs.</li> <li><code>ExternalMediaConnector</code> \u2014 YouTube, podcasts, RSS, S3/GCS, etc., as in SEEDS's ExternalSource model and audioDownloaderService.</li> <li><code>IVRRecordingConnector</code> \u2014 SEEDS' IVR recordings; uses IVR backend IDs.</li> </ul>"},{"location":"architecture/universal-knowledge-ingestion/#processing-pipeline-layer-agents-tools","title":"Processing Pipeline Layer (Agents &amp; Tools)","text":"<p>We define a Pipeline Orchestrator that runs a graph of steps, where each step is an <code>IngestionAgent</code> or/and <code>IngestionTool</code> with explicit input/output contracts. This reuses the agent &amp; tool library philosophy from the Inclusive Architecture.</p> <p>Common Agents and Tool types</p> <ol> <li> <p>Parser Agents</p> <ul> <li><code>DocumentParserAgent</code>: (PDF, DOCX \u2192 markdown/structured JSON)<ul> <li>Implements Shiksha's TOC discovery, splitting, and LLM/vision-based extraction as sub-steps.</li> <li>Supports alternative engines (MinerU, OCR).</li> </ul> </li> <li><code>TextParserAgent</code>: (plain text, HTML, FAQs \u2192 normalized text). Generalizes BYOEB's LlamaIndex text parsing.</li> <li><code>AudioParserAgent</code>: (audio \u2192 transcription + timestamps). Wraps SEEDS' TranscriptionService (Azure STT).</li> </ul> </li> <li> <p>Cleaning &amp; Normalization Agents</p> <ul> <li><code>MarkdownCleaningAgent</code> \u2014 Shiksha's TextCleaningStep (LLM-powered cleaning) generalized to all markdown sources.</li> <li><code>TranscriptionPostProcessorAgent</code> \u2014 punctuation, diarization, noise trimming, language detection for audio transcripts.</li> </ul> </li> <li> <p>Enrichment &amp; Domain Agents</p> <ul> <li><code>LearningOutcomeExtractionAgent</code> \u2014 Shiksha LO extraction steps (chapter-level, subtopic-level) exposed as a reusable agent that works on any curriculum-like content.</li> <li><code>KnowledgeGraphBuilderAgent</code> \u2014 uses Shiksha KG steps (entities, relationships, Neo4j export).</li> <li><code>FAQGenerationAgent</code> \u2014 optional; generate FAQs for BYOEB or SEEDS from long docs/audio summaries.</li> <li><code>KeywordAndTaggingAgent</code> \u2014 used in SEEDS to derive keywords/tags; can be reused elsewhere.</li> </ul> </li> <li> <p>Chunking Agents</p> <ul> <li><code>SentenceChunkingAgent</code> \u2014 wraps BYOEB's LlamaIndex SentenceSplitter with configurable chunk_size / overlap.</li> <li><code>PageChunkingAgent</code> \u2014 Shiksha's page-based splitting for textbooks.</li> <li><code>TimedChunkingAgent</code> \u2014 for audio segments (based on transcript timestamps) to create semantically and temporally aligned chunks for SEEDS.</li> </ul> <p>How Time-Based Chunking Works:</p> <ol> <li>The <code>AudioParserAgent</code> produces a transcript with word-level or segment-level timestamps (e.g., from Azure STT's <code>word_timings</code>).</li> <li><code>TimedChunkingAgent</code> receives this timestamped transcript and applies configurable rules:<ul> <li>Fixed duration windows: Split every N seconds (e.g., 30s chunks) with optional overlap.</li> <li>Silence/pause detection: Use natural pauses (&gt;500ms) as chunk boundaries.</li> <li>Semantic + temporal hybrid: Combine sentence boundaries with time limits \u2014 prefer splitting at sentence ends, but force-split if duration exceeds max (e.g., 60s).</li> </ul> </li> <li>Each resulting chunk carries metadata: <code>{ \"start_time\": 45.2, \"end_time\": 72.8, \"duration\": 27.6 }</code>.</li> <li>These timestamps enable audio seek during playback in SEEDS IVR/teacher experiences \u2014 users can jump to the relevant audio segment for any retrieved chunk.</li> </ol> </li> <li> <p>Embedding &amp; Indexing Agents</p> <ul> <li><code>EmbeddingAgent</code> \u2014 wraps any embedding model; configuration chooses:<ul> <li>Provider: Azure OpenAI (current), OpenAI API, local model, etc.</li> <li>Model: <code>text-embedding-3-large</code>, others; embedding dimension recorded in metadata.</li> </ul> </li> <li><code>VectorIndexingAgent</code> \u2014 writes to pluggable vector stores.</li> <li><code>GraphIndexingAgent</code> \u2014 writes to Neo4j (for curriculum KG).</li> <li><code>KeywordIndexingAgent</code> \u2014 writes full-text indexes (e.g., Mongo text indexes used in SEEDS).</li> </ul> </li> <li> <p>Governance &amp; Logging Agents</p> <ul> <li><code>IngestionLoggerAgent</code> \u2014 logs events, durations, outcomes (success/fail).</li> <li><code>DataLineageAgent</code> \u2014 persistent record with full traceability of the chain including sources, pipeline and versions, LLMs, vector store, etc. This enables reproducibility, impact analysis, audit compliance and debugging.</li> <li><code>QualityVerifierAgent</code> \u2014 sample-based QA (e.g., compare extracted text with original pages; check LO consistency).</li> </ul> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-orchestration","title":"Pipeline Orchestration","text":"<p>Static Orchestration with LLM-Powered Steps:</p> <p>The ingestion framework uses declarative, static pipelines \u2014 NOT LLM-driven orchestration. This is a deliberate design choice:</p> Aspect Our Approach Alternative (LLM Orchestration) Step sequencing Defined in YAML, runs sequentially LLM decides next step dynamically LLM usage Within individual steps (parsing, cleaning, extraction) Also for routing/decision-making Predictability High \u2014 same input produces same flow Variable \u2014 LLM may choose different paths Cost Lower \u2014 LLM calls only where needed Higher \u2014 additional reasoning calls Debuggability Easy \u2014 trace through known steps Harder \u2014 must understand LLM's decisions Auditability Full \u2014 pipeline version = exact behavior Partial \u2014 depends on LLM reasoning logs <p>Why static orchestration?</p> <ul> <li>Ingestion is a batch/background process where predictability and reproducibility matter more than flexibility.</li> <li>Each step already uses LLMs where valuable (vision-based parsing, text cleaning, LO extraction).</li> <li>Pipeline versioning ensures identical behavior across runs \u2014 critical for compliance and debugging.</li> <li>Cost-efficient: No \"reasoning overhead\" for routing decisions.</li> </ul> <p>Example: YAML, JSON of a pipeline described as declarative configs <pre><code>pipeline_id: shiksha_textbook_v1\ndomain_profile: education_shiksha\ninput_modality: document\nsteps:\n  - agent: DocumentParserAgent\n    config: { strategy: \"llm_vision\" }\n  - agent: MarkdownCleaningAgent\n  - agent: LearningOutcomeExtractionAgent\n  - agent: SentenceChunkingAgent\n    config: { chunk_size: 1024, overlap: 100 }\n  - agent: EmbeddingAgent\n    config: { model: \"text-embedding-3-large\", provider: \"azure\" }\n  - agent: VectorIndexingAgent\n    config: { store: \"qdrant\" }\n  - agent: KnowledgeGraphBuilderAgent\n    config: { graph_store: \"neo4j\" }\n</code></pre></p> <p>BYOEB\u2019s current <code>KBService.upload()</code> orchestration (download \u2192 parse \u2192 similarity check \u2192 add to vector store) becomes one such pipeline (health_kb_v1), and its BaseVectorStore abstraction can be moved into the shared library.</p> <p>SEEDS\u2019 \u201caudio \u2192 transcription \u2192 keywords \u2192 embedding \u2192 KnowledgeItem\u201d flow becomes <code>accessibility_audio_v1</code>.</p>"},{"location":"architecture/universal-knowledge-ingestion/#storage-registration-layer","title":"Storage &amp; Registration Layer","text":"<p>Storage Layer</p> <p>1. Object Storage</p> <pre><code>interface ObjectStore {\n  put(bytes, key, metadata?): Promise&lt;string&gt;;  // returns URI\n  get(key): Promise&lt;RawArtifact&gt;;\n}\n</code></pre> <ul> <li>Implementation: Azure Blob (existing in BYOEB &amp; SEEDS).</li> <li>Used for PDF chapters, processed markdown, audio files, etc.</li> </ul> <p>2. Vector Store</p> <pre><code>interface VectorStore {\n  upsert(vectors: VectorRecord[]): Promise&lt;void&gt;;\n  search(query_vector, k, filters?): Promise&lt;VectorRecord[]&gt;;\n  count(): Promise&lt;number&gt;;\n}\n</code></pre> <ul> <li>Backends:<ul> <li>Azure Vector Search (BYOEB).</li> <li>Qdrant + Azure Vector Search (Shiksha).</li> <li>Chroma / pgvector for local/offline scenarios.</li> </ul> </li> <li>Reuse BYOEB's BaseVectorStore design and vector upsert/similarity logic (including 0.95 similarity de-duplication).</li> </ul> <p>3. Graph Store</p> <pre><code>interface GraphStore {\n  upsertNodes(nodes: Node[]);\n  upsertEdges(edges: Edge[]);\n}\n</code></pre> <ul> <li>Neo4j backing for Shiksha's KG export.</li> </ul> <p>4. Metadata Store</p> <ul> <li>Prefer a common metadata/entity store (e.g., Mongo/Postgres) that holds <code>KnowledgeItem</code> docs referencing object/vector/graph layers.</li> <li>SEEDS' <code>KnowledgeItem</code> model is a good starting point, extended slightly to match the canonical KnowledgeItem spec.</li> </ul> <p>5. Registration and Discovery</p> <ul> <li>After ingestion, each pipeline registers a <code>KnowledgeItem</code> (or batch) into a Knowledge Registry:<ul> <li>Saves core attributes (domain_profile, modality, metadata).</li> <li>Records pointers to:<ul> <li>Blob URIs for raw/processed files.</li> <li>Vector store IDs / collections.</li> <li>Graph DB node IDs.</li> </ul> </li> <li>Agents (e.g., RAG agents in Experience Layer) ask a KnowledgeBaseAdapter tool for:<ul> <li><code>get_items(domain_profile, filters)</code></li> <li><code>semantic_search(query, profile, k, filters)</code></li> <li><code>get_by_id(id)</code></li> </ul> </li> </ul> </li> </ul> <p>This is the single consistent entrypoint for BYOEB chat, Shiksha copilot, and SEEDS IVR/teacher experiences to reuse the same knowledge.</p>"},{"location":"architecture/universal-knowledge-ingestion/#domain-profiles","title":"Domain Profiles","text":"<p>Domain profiles use YAML schema with multi tenant isolation &amp; secrets</p> <ol> <li> <p>YAML: Public profiles live in git and describe what to do, not how to authenticate.</p> </li> <li> <p>Secretful Config (private): A separate, non-public config (env or private YAML) maps these logical refs to real credentials.</p> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#multi-tenant-support","title":"Multi-Tenant Support","text":"<p>Every pipeline run is created with a tenant. The runner uses: - <code>tenant.slug</code> plus template strings in YAML to produce:     - Vector index name / collection.     - Graph DB database/namespace. - Postgres tables always carry tenant_id, so queries are scoped.</p> <p>That gives: - Logical separation in all stores. - Ability to run a single profile across many tenants.</p> <p>Tenant\u2013Pipeline Relationship:</p> <p>A single tenant can use multiple pipelines simultaneously. For example, a state education board tenant (<code>kseeb_grade6</code>) might use: - <code>shiksha_textbook_v1</code> for textbook ingestion - <code>shiksha_audio_v1</code> for supplementary audio content - <code>generic_faq_v1</code> for FAQ documents</p> <p>The <code>pipeline_runs</code> table links each run to both <code>tenant_id</code> and <code>pipeline_version_id</code>, so: - Tenants can mix-and-match pipelines as needed. - Each pipeline can be versioned independently. - Retrieval queries can filter by <code>domain_profile</code> or span across all knowledge for that tenant.</p> <p>Pipeline YAML configs are versioned in git (e.g., <code>profiles/shiksha_textbook_v1.yaml</code>). The <code>pipeline_versions</code> table's <code>yaml_profile_name</code> field references this file. When a pipeline evolves, we create a new version (v1.1, v2.0) rather than modifying the existing YAML, ensuring reproducibility and auditability.</p>"},{"location":"architecture/universal-knowledge-ingestion/#how-this-hooks-back-into-the-orchestration-layer","title":"How This Hooks Back into the Orchestration Layer","text":"<p>From the Orchestrator's point of view, Ingestion is just another tool:</p> <ul> <li>Tool: <code>ingest_content</code><ul> <li>Inputs: { <code>tenant_slug</code>, <code>pipeline_name</code>, <code>pipeline_version?</code> , <code>source_uri</code>, <code>extra_params</code> }</li> <li>Output: { <code>pipeline_run_id</code>, <code>knowledge_item_ids[]</code> }</li> </ul> </li> <li>Tool: <code>search_knowledge</code><ul> <li>Inputs: { <code>tenant_slug</code>, <code>domain_profile</code>, <code>query</code>, <code>filters</code> } </li> <li>Uses Postgres + configured vector store to:<ul> <li>pick the right indexes (<code>vector_store_space</code>)</li> <li>call vector search</li> <li>hydrate results from <code>chunks</code> and <code>knowledge_items</code>.</li> </ul> </li> </ul> </li> </ul> <p>This gives one Knowledge Base interface for all three systems &amp; any new ones.</p>"},{"location":"architecture/universal-knowledge-ingestion/#backward-compatibility-versioning-strategy","title":"Backward Compatibility &amp; Versioning Strategy","text":""},{"location":"architecture/universal-knowledge-ingestion/#register-current-workflows-as-v0x-pipelines","title":"Register Current Workflows as v0.x Pipelines","text":"<p>BYOEB - Treat the current <code>KBService.upload()</code> + Azure Blob download as pipeline byoeb_kb version 0.1. - The FastAPI endpoint <code>/vector/index</code> continues to call kb_upload(). Internally, we:     - Call the new PipelineRunner with <code>pipeline_version='0.1'</code>.     - Use the <code>BYOEBKBUploadStep</code> adapter which mostly just wraps existing behaviour.     - Pipe logs into Postgres <code>pipeline_runs</code> / <code>pipeline_step_runs</code> as well as existing logs.</p> <p>Shiksha Copilot - The current <code>pipeline_runner.py</code> config is effectively pipeline <code>shiksha_textbook</code> version <code>0.1</code>. - We wrap its steps to conform to the new Step protocol and run it under the Ingestion Service for new ingestions while still allowing CLI-style runs.</p> <p>SEEDS No live code yet, so first implementation can start directly as seeds_audio version 1.0.</p>"},{"location":"architecture/universal-knowledge-ingestion/#introducing-v1x-pipelines","title":"Introducing v1.x Pipelines","text":"<p>For each domain we can then evolve:</p> <p>byoeb_kb v1.0: Maybe adds translation agent, or uses a different parser. In new framework:</p> <p>Connectors:</p> <ul> <li><code>AzureBlobConnector</code> using existing async blob client.</li> </ul> <p>Steps:</p> <ul> <li><code>BYOEBListFilesStep</code> (optional) \u2192 lists &amp; filters FileMetadata.</li> <li><code>BYOEBDownloadStep</code> wrapping <code>_abulk_download_files</code>.</li> <li><code>BYOEBParseChunksStep</code> wrapping <code>text_parser.get_chunks_from_collection(...)</code>.</li> <li><code>BYOEBUpsertStep</code> wrapping <code>_gather_similar_chunks</code> + <code>_add_nodes_to_vector_store</code>.</li> </ul> <p>shiksha_textbook v1.0: Moves to Postgres-backed registry; uses improved chunking for better RAG. In new framework:</p> <p>Each of Steps 3\u20138 (text extraction, cleaning, LO extraction, subtopic cleaning, subtopic-wise LO extraction, create index) becomes a <code>Step</code> in the Ingestion Service.</p> <p>The <code>CreateIndexStep</code> additionally:</p> <ul> <li>Writes <code>knowledge_items</code> row for the chapter, with metadata <code>{board, grade, subject, chapter_number, medium}</code>.</li> <li>Writes <code>chunks</code> rows for each page/subtopic chunk with Qdrant pointers.</li> </ul> <p>Optional KG steps become a separate pipeline (<code>shiksha_kg_v1</code>) that reads from the same cleaned markdown and writes to the graph store.</p> <p>The Shiksha \"experience layer\" agents then call the KnowledgeBase tool using board/grade/subject or a combination of all 3 filters that are now available across all stores and Postgres.</p> <p>seeds_audio v1.0: First implementation. Because each <code>knowledge_item</code> row carries <code>pipeline_version_id</code>, retrieval logic can behave differently if necessary (e.g., interpret metadata shapes slightly differently). In new framework:</p> <ul> <li><code>ExternalSource</code> + <code>audioDownloaderService</code> \u2192 <code>SourceConnector</code> implementations.</li> <li>Transcription and keyword extraction steps \u2192 agents in the pipeline.</li> <li><code>KnowledgeItem</code> DB model becomes a tenant-specific view of the generic <code>knowledge_items</code> table (we can keep a Mongo version or migrate to Postgres gradually).</li> </ul> <p>The canonical <code>knowledge_items</code> row for an audio piece looks like: <pre><code>{\n  \"domain_profile\": \"accessibility_seeds\",\n  \"modality\": \"audio\",\n  \"source_uri\": \"blob://seeds/audio/123.mp3\",\n  \"metadata\": {\n    \"language\": \"kn-IN\",\n    \"theme\": \"math lesson\",\n    \"type\": \"story\",\n    \"duration\": 300,\n    \"tags\": [\"grade6\", \"fractions\"],\n    \"transcript_available\": true\n  }\n}\n</code></pre></p> <p>And chunk rows correspond to transcript segments, with timestamps in <code>metadata</code>.</p> <p>generic_document v1.0: A starter profile for new applications onboarding onto the ingestion framework.</p> <p>This profile provides sensible defaults that work for most text/document use cases without requiring domain-specific customization:</p> <pre><code>pipeline_id: generic_document_v1\ndomain_profile: generic\ninput_modality: document\ndescription: \"Default pipeline for new applications - handles PDFs, DOCX, text files\"\nsteps:\n  - agent: DocumentParserAgent\n    config: \n      strategy: \"auto\"  # auto-detect: use OCR for scanned, direct extraction for digital\n      fallback_to_ocr: true\n  - agent: MarkdownCleaningAgent\n    config:\n      preserve_tables: true\n      preserve_lists: true\n  - agent: SentenceChunkingAgent\n    config: \n      chunk_size: 512\n      overlap: 50\n  - agent: EmbeddingAgent\n    config: \n      model: \"text-embedding-3-small\"  # cost-effective default\n      provider: \"azure\"\n  - agent: VectorIndexingAgent\n    config: \n      store: \"qdrant\"  # or \"${tenant.default_vector_store}\"\n</code></pre> <p>New applications can:</p> <ol> <li>Start with generic_document_v1 to get basic ingestion working immediately.</li> <li>Fork and customize \u2014 copy the YAML, add domain-specific agents (e.g., <code>FAQGenerationAgent</code>), adjust chunking parameters.</li> <li>Graduate to a named profile \u2014 once stable, register as <code>myapp_kb_v1</code> with domain_profile <code>myapp</code>.</li> </ol> <p>This lowers the barrier to onboarding while maintaining the full power of the framework for teams ready to customize.</p>"},{"location":"architecture/universal-knowledge-ingestion/#low-level-design","title":"Low Level Design","text":""},{"location":"architecture/universal-knowledge-ingestion/#postgres-metadata-registry-design","title":"Postgres Metadata &amp; Registry Design","text":"<p>We\u2019ll put the \u201cbrain\u201d of ingestion metadata in Postgres and keep vectors in the vector stores.</p>"},{"location":"architecture/universal-knowledge-ingestion/#core-tables","title":"Core Tables","text":"<p>tenants <pre><code>CREATE TABLE tenants (\n  id              UUID PRIMARY KEY,\n  slug            TEXT UNIQUE NOT NULL,  -- e.g. 'asha_raj', 'kseeb_grade6'\n  display_name    TEXT NOT NULL,\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipelines \u2013 logical pipelines (e.g. byoeb_kb, shiksha_textbook, seeds_audio) <pre><code>CREATE TABLE pipelines (\n  id              UUID PRIMARY KEY,\n  name            TEXT NOT NULL,           -- 'byoeb_kb', 'shiksha_textbook'\n  domain_profile  TEXT NOT NULL,           -- 'health_byoeb', 'education_shiksha', ...\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipeline_versions <pre><code>CREATE TABLE pipeline_versions (\n  id                UUID PRIMARY KEY,\n  pipeline_id       UUID REFERENCES pipelines(id),\n  version           TEXT NOT NULL,          -- '0.1', '1.0', '1.1'\n  yaml_profile_name TEXT NOT NULL,          -- name of the YAML in git\n  is_default        BOOLEAN NOT NULL DEFAULT false,\n  created_at        TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipeline_runs \u2013 like Shiksha\u2019s pipeline runner, but shared and tenant-aware <pre><code>CREATE TABLE pipeline_runs (\n  id                UUID PRIMARY KEY,\n  tenant_id         UUID REFERENCES tenants(id),\n  pipeline_version_id UUID REFERENCES pipeline_versions(id),\n  source_uri        TEXT NOT NULL,  -- blob path, http URL, etc.\n  status            TEXT NOT NULL,  -- 'queued','running','succeeded','failed'\n  started_at        TIMESTAMPTZ,\n  completed_at      TIMESTAMPTZ,\n  error_message     TEXT\n);\n</code></pre></p> <p>pipeline_step_runs \u2013 thin wrapper around Shiksha\u2019s StepResult <pre><code>CREATE TABLE pipeline_step_runs (\n  id                UUID PRIMARY KEY,\n  pipeline_run_id   UUID REFERENCES pipeline_runs(id),\n  step_name         TEXT NOT NULL,          -- 'text_extraction_llm', 'kb_upload', ...\n  status            TEXT NOT NULL,          -- matches StepStatus\n  output_paths      JSONB,                  -- type-&gt;path (for file-based outputs)\n  metadata          JSONB,\n  error_message     TEXT,\n  started_at        TIMESTAMPTZ,\n  completed_at      TIMESTAMPTZ\n);\n</code></pre></p> <p>knowledge_items \u2013 canonical registry entry <pre><code>CREATE TABLE knowledge_items (\n  id                  UUID PRIMARY KEY,\n  tenant_id           UUID REFERENCES tenants(id),\n  domain_profile      TEXT NOT NULL,\n  modality            TEXT NOT NULL,            -- 'text','document','audio','mixed'\n  source_uri          TEXT NOT NULL,            -- blob/URL/IVR ID\n  pipeline_version_id UUID REFERENCES pipeline_versions(id),\n  title               TEXT,\n  description         TEXT,\n  metadata            JSONB,                    -- board, grade, subject, tags...\n  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),\n  updated_at          TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>chunks \u2013 registry for text/semantic units (BYOEB\u2019s Chunk / LlamaIndex TextNode) <pre><code>CREATE TABLE chunks (\n  id                  TEXT PRIMARY KEY,         -- reuse chunk_id/node_id\n  knowledge_item_id   UUID REFERENCES knowledge_items(id),\n  tenant_id           UUID REFERENCES tenants(id),\n  text                TEXT NOT NULL,\n  metadata            JSONB,\n  vector_store_kind   TEXT NOT NULL,            -- 'azure_vector_search','qdrant',...\n  vector_store_space  TEXT NOT NULL,            -- index/collection/namespace name\n  vector_store_ref_id TEXT NOT NULL,            -- id/primary key inside store\n  created_at          TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>We don\u2019t store the vector in Postgres; we just store the pointer to wherever that vector lives.</p> <p>For SEEDS, a <code>KnowledgeItem</code> will additionally carry transcription bits in metadata or a sibling audio_segments table later.</p>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-step-abstractions","title":"Pipeline &amp; Step Abstractions","text":"<p>Step Interface</p> <p>We unify BYOEB\u2019s monolithic <code>KBService.upload()</code> and Shiksha\u2019s multi-step pipeline by introducing a generic Step that always returns a Shiksha-style StepResult. <pre><code>class Step(Protocol):\n    name: str\n\n    async def run(self, ctx: IngestionContext) -&gt; StepResult:\n        ...\n</code></pre></p> <p>Where: <pre><code>@dataclass\nclass StepResult:   # from Shiksha, reused\n    status: StepStatus\n    output_paths: Dict[str, str] | None = None\n    error: Exception | None = None\n    metadata: Dict[str, Any] | None = None\n</code></pre></p> <p><code>IngestionContext</code> is a mutable object holding: - tenant, pipeline_version, pipeline_run_id - references to raw artifacts - intermediate outputs (e.g., extracted markdown path) - a Postgres connection and store registries.</p> <p>Step Types</p> <p>We\u2019ll create adapters so existing code can be treated as Steps.</p> <p>-BYOEB KB Upload Step</p> <p>Wrap the existing <code>KBService.upload()</code> to behave like a pipeline step.</p> <pre><code>class BYOEBKBUploadStep(Step):\n    name = \"byoeb_kb_upload\"\n\n    async def run(self, ctx: IngestionContext) -&gt; StepResult:\n        kb_service = KBService(\n            vector_store=ctx.stores.vector_store,\n            media_storage=ctx.stores.media_storage,\n            llm_client=ctx.models.llm,\n            upsert_t=ctx.config.upsert_threshold,\n        )\n        count = await kb_service.upload(ctx.inputs.files_metadata)\n        return StepResult(\n            status=StepStatus.SUCCESS,\n            metadata={\"chunks_ingested\": count}\n        )\n</code></pre> <p>The orchestrator then records a pipeline_step_runs row with this metadata.</p> <p>-Shiksha Steps</p> <p>Each existing Shiksha Step (TOC, text extraction, text cleaning, LO extraction, index creation, KG) already conforms conceptually: they write outputs to files and can populate <code>StepResult.output_paths</code>.</p> <p>We just ensure: - They accept an <code>IngestionContext</code>. - They fill <code>StepResult</code> and allow the orchestrator to record them.</p> <p>-SEEDS Steps</p> <p>For SEEDS, we\u2019ll add steps like: - <code>AudioDownloadStep</code> (pull from blob/external URL) - <code>TranscriptionStep</code> - <code>KeywordExtractionStep</code> - <code>AudioChunkingStep</code> - <code>EmbeddingIndexingStep</code> - <code>KnowledgeItemRegistrationStep</code></p> <p>All of them return <code>StepResult</code>, write rows to Postgres, and index vectors appropriately.</p>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-orchestrator","title":"Pipeline Orchestrator","text":"<p>The Ingestion Service runs pipeline versions from YAML configs using these steps. We keep it conceptually simple: <pre><code>class PipelineRunner:\n    async def run(self, tenant, pipeline_version, source_uri, params):\n        run_id = insert_pipeline_run(...)\n        ctx = IngestionContext(...)\n\n        for step in pipeline_version.steps:\n            res = await step.run(ctx)\n            insert_pipeline_step_run(run_id, step.name, res)\n            if res.status is StepStatus.FAILURE:\n                mark_run_failed(run_id, res.error)\n                break\n\n        # On success, create KnowledgeItem rows, Chunk rows, etc.\n</code></pre></p> <p>This is where backward compatibility lives \u2013 old workflows become specific pipeline versions.</p>"},{"location":"architecture/universal-knowledge-ingestion/#future-extensions","title":"Future Extensions","text":"<p>Image &amp; Multimodal Ingestion - Vision OCR for handwritten content and diagrams - Object/diagram extraction and description generation - Multimodal embeddings (e.g., CLIP-style) for image+text retrieval</p> <p>Multi-Lingual Content Handling - Language detection at ingestion time (auto-tag <code>metadata.language</code>) - Language-specific chunking rules (e.g., different tokenization for Hindi/Kannada vs English) - Cross-lingual embeddings or language-specific embedding models - Translation agents for creating parallel content in multiple languages - Script handling (Devanagari, Kannada script) for audio transcription post-processing</p> <p>MCP-Based Ingestion Triggers - Expose ingestion as an MCP (Model Context Protocol) server, allowing LLM agents to directly invoke <code>ingest_content</code> and <code>search_knowledge</code> tools - Enable agentic workflows where an AI assistant can autonomously decide to ingest new content (e.g., \"I found a relevant PDF, let me add it to the knowledge base\") - Support MCP resource discovery \u2014 agents can list available pipelines and their capabilities</p> <p>Cross-Domain Knowledge Graph - Unified graph that connects health, education, and accessibility knowledge - Enable cross-domain queries (e.g., \"health topics relevant to grade 6 science curriculum\") - Shared entity resolution across domains</p> <p>Pipeline Selector Agent - For highly dynamic content (e.g., mixed-format uploads where the optimal pipeline varies), introduce an optional <code>PipelineSelectorAgent</code> - Uses an LLM to analyze incoming content and select the most appropriate pipeline (e.g., \"this looks like a scanned textbook \u2192 use <code>shiksha_textbook_v1</code>\" vs \"this is a plain FAQ document \u2192 use <code>generic_document_v1</code>\") - The selected pipeline still runs statically \u2014 LLM only assists in the initial routing decision - Useful for self-service upload portals where users don't specify content type</p>"},{"location":"concepts/AI-Governance/","title":"Agentic Governance &amp; Safety Framework","text":"<p>A Business Case for Responsible, Inclusive AI</p>"},{"location":"concepts/AI-Governance/#the-problem-ai-without-guardrails-is-a-business-risk","title":"The Problem: AI Without Guardrails is a Business Risk","text":"<p>Organizations deploying AI\u2014especially in sensitive domains like healthcare, education, and public services\u2014face a growing dilemma:</p> Risk Business Impact AI says something wrong or harmful Reputation damage, user harm, legal liability Compliance violations Regulatory fines, loss of operating license, audit failures Unpredictable costs AI usage bills spiral; CFO asks \"who approved this?\" No audit trail When something goes wrong, you can't explain what happened Bias and exclusion AI serves some users well, fails others\u2014especially vulnerable populations <p>The hard truth: Most AI systems today are deployed with minimal governance. Teams move fast, ship features, and hope nothing goes wrong. When it does, the scramble begins.</p> <p>For inclusive AI\u2014systems serving health workers in rural areas, visually impaired learners, or underserved communities\u2014the stakes are even higher. These users often have no alternative. If the AI fails them, they're left with nothing.</p>"},{"location":"concepts/AI-Governance/#our-vision-ai-you-can-trust","title":"Our Vision: AI You Can Trust","text":"<p>The Agentic Governance &amp; Safety Framework is a foundational layer of the Inclusive Architecture that ensures every AI interaction is:</p> Principle What It Means Architecture Reference Safe Harmful, misleading, or inappropriate content is caught before it reaches users Guard Layer Compliant Healthcare, education, and data privacy regulations are enforced automatically Engine Layer Accountable Every AI decision has a clear audit trail\u2014who, what, when, why Audit Engine Cost-Controlled Budgets are respected; no surprise bills; intelligent resource allocation Cost Engine Inclusive Governance rules ensure AI serves all users equitably, especially the underserved Policy &amp; Configuration Layer"},{"location":"concepts/AI-Governance/#what-this-means-for-your-organization","title":"What This Means for Your Organization","text":""},{"location":"concepts/AI-Governance/#for-healthcare-deployments","title":"For Healthcare Deployments","text":"<ul> <li>AI never provides diagnoses\u2014it shares information and always recommends professional consultation</li> <li>Emergency situations are immediately escalated to human responders</li> <li>Patient data handling follows local compliance and privacy regulations by design (PHI detection &amp; anonymization)</li> <li>Every health-related AI response is logged for clinical audit (Audit Engine)</li> <li>See Healthcare Domain Profile for complete healthcare compliance configuration</li> </ul>"},{"location":"concepts/AI-Governance/#for-education-deployments","title":"For Education Deployments","text":"<ul> <li>Content is age-appropriate and curriculum-aligned (Minor Data Guard)</li> <li>Student data is protected; parents and institutions maintain control (Parental Consent Guard)</li> <li>AI adapts difficulty without frustrating or discouraging learners</li> <li>Teacher oversight is preserved\u2014AI assists, never replaces</li> <li>See Education Domain Profile for complete education compliance configuration</li> </ul>"},{"location":"concepts/AI-Governance/#for-accessibility-inclusion","title":"For Accessibility &amp; Inclusion","text":"<ul> <li>AI doesn't assume all users have smartphones or high-speed internet</li> <li>Responses are validated to work across channels: voice, text, Braille</li> <li>Cost controls ensure AI resources are distributed equitably across user segments (Cost Engine)</li> <li>Bias detection ensures underserved communities receive the same quality of service (Safety Engine)</li> </ul>"},{"location":"concepts/AI-Governance/#the-business-case","title":"The Business Case","text":"Without Governance With Governance Framework React to incidents after they happen Prevent incidents before they occur (Request Guards) Manual compliance audits (expensive, slow) Continuous automated compliance (Audit Engine) AI costs are unpredictable Predictable budgets with intelligent cost routing (Cost Engine, LLM Gateway) Blame game when things go wrong Clear accountability and decision trails (GovernanceContext) Accessibility is a \"nice to have\" Inclusion is enforced by design (Domain Profiles) Scaling AI = scaling risk Scaling AI = scaling trust (Multi-Tenant Support)"},{"location":"concepts/AI-Governance/#key-outcomes","title":"Key Outcomes","text":"Metric Expected Impact Implementation Safety incidents \u2193 90% reduction in harmful AI outputs reaching users Guard Layer + Safety Engine Compliance readiness Audit-ready from day one; \u2193 60% time to certification Audit Engine + Database Architecture AI costs \u2193 30% through intelligent model selection and budget enforcement Cost Engine + LLM Gateway Routing User trust \u2191 NPS/CSAT scores as users experience consistent, reliable AI Governance Pipeline Time to market \u2191 Faster deployment because governance is built-in, not bolted-on Domain Profiles"},{"location":"concepts/AI-Governance/#why-this-matters-for-inclusive-ai","title":"Why This Matters for Inclusive AI","text":"<p>Traditional AI governance frameworks are built for enterprise chatbots and consumer apps. They assume users have:</p> <ul> <li>Reliable internet connectivity</li> <li>Smartphones or computers</li> <li>Ability to read and respond to disclaimers</li> <li>Alternative channels if AI fails</li> </ul> <p>Our users often have none of these.</p> <p>A health worker in rural Rajasthan using WhatsApp to access medical information. A visually impaired student in Karnataka using an IVR system to learn mathematics. A community volunteer using a feature phone to report local health data.</p> <p>For these users, AI governance isn't just about protecting the organization\u2014it's about protecting the people who have no alternative.</p> <p>The Agentic Governance &amp; Safety Framework ensures that inclusive AI is also responsible AI:</p> <ul> <li>If the AI doesn't know, it says so\u2014no confident-sounding hallucinations (Hallucination Detection Guard)</li> <li>If the situation is serious, humans are involved\u2014escalation is automatic, not optional (Guard Escalation)</li> <li>If resources are limited, they're allocated fairly\u2014not just to high-value users (Cost Engine)</li> <li>If something goes wrong, we can explain what happened\u2014and improve (Audit Engine)</li> </ul>"},{"location":"concepts/AI-Governance/#the-bottom-line","title":"The Bottom Line","text":"<p>\"Move fast and break things\" doesn't work when you're serving vulnerable populations.</p> <p>The Agentic Governance &amp; Safety Framework enables organizations to deploy AI that is:</p> <ul> <li>Bold in its ambition to serve underserved communities</li> <li>Responsible in how it handles sensitive information and high-stakes decisions</li> <li>Sustainable in its cost structure and operational model</li> <li>Trustworthy in the eyes of users, regulators, and partners</li> </ul>"},{"location":"concepts/AI-Governance/#call-to-action","title":"Call to Action","text":"<p>As you scale AI across healthcare, education, and accessibility:</p> <ol> <li>Don't bolt on governance later\u2014build it into the foundation</li> <li>Don't treat compliance as a checkbox\u2014make it continuous and automatic</li> <li>Don't assume your users can recover from AI failures\u2014design for the most vulnerable</li> <li>Don't let AI costs surprise you\u2014govern them proactively</li> </ol> <p>The Inclusive Architecture's Governance &amp; Safety Framework gives you all of this\u2014so you can focus on impact, not damage control.</p>"},{"location":"concepts/AI-Governance/#related-concepts","title":"Related Concepts","text":"<ul> <li>Inclusive Architecture \u2014 The overarching architectural template</li> <li>Universal Knowledge Ingestion \u2014 How knowledge enters the system safely</li> </ul>"},{"location":"concepts/AI-Governance/#architecture-documentation","title":"Architecture Documentation","text":"<p>For detailed technical implementation, see:</p> <ul> <li>Agentic Governance &amp; Safety Framework \u2014 Complete architecture, engines, guards, and implementation details</li> <li>Guard Layer \u2014 Request and response guards</li> <li>Engine Layer \u2014 Consent, Cost, Safety, and Audit engines</li> <li>Domain Profiles \u2014 Healthcare, education, and accessibility configurations</li> <li> <p>Database Architecture \u2014 Storage and audit trail design</p> </li> <li> <p>LLM Gateway Layer \u2014 Foundational infrastructure for LLM access</p> </li> <li>Cost Tracking &amp; Budget Management \u2014 Per-request cost calculation and budgets</li> <li>Guardrails &amp; Content Safety \u2014 Built-in PII detection and content moderation</li> <li>Integration with Governance Framework \u2014 How Gateway and Governance work together</li> </ul> <p>\u00a9 2024 A4I. Part of the Inclusive Agentic Architecture.</p>"}]}
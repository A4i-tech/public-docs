{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"onboarding/","title":"\ud83d\udc4b Welcome to the A4I Development Team","text":"<p>We\u2019re excited to have you as part of our open-source vision to create social good through technology!  </p> <p>This guide covers: 1. General guidelines &amp; expectations 2. Communications setup 3. Development environment 4. A4I impact areas (Healthcare, Education, Accessibility)</p>"},{"location":"onboarding/#1-general-guidelines","title":"1. \u2705 General Guidelines","text":""},{"location":"onboarding/#tech-proficiency","title":"Tech Proficiency","text":"<p>While A4I has impact across multiple sectors through solutions, we expect new contributors to be comfortable with: - Version Control: Git, GitHub (PRs, Issues, branching) - Programming Basics: Concepts of OS, Networking, Data Structures and Algorithm - Collaboration Tools: Markdown, documentation practices, AI / non-AI based IDEs - Cloud &amp; Infra (Basics):  Docker, CI/CD concepts via Github Actions, knowledge of working on cloud -  Microsoft Azure, AWS or GCP</p> <p>\u26a0\ufe0f If you\u2019re unfamiliar with any of these, don\u2019t worry\u2014pairing and resource links are available.</p>"},{"location":"onboarding/#values-we-uphold","title":"Values We Uphold","text":"<ul> <li>Open-source collaboration</li> <li>Accessibility-first design</li> <li>Clear communication &amp; documentation</li> <li>Regular sprint planning and retrospectives</li> </ul>"},{"location":"onboarding/#2-communications-setup","title":"2. \ud83d\udcac Communications Setup","text":"<ol> <li>Email Access </li> <li>Ensure you have your <code>@iiitb.ac.in</code> email account set up with Outlook access.</li> <li> <p>Add it to GitHub for repo access + notifications.</p> </li> <li> <p>Microsoft Teams </p> </li> <li>Join the <code>A4I IIITB</code> team.  </li> <li>Channels include:  <ul> <li><code>General</code> \u2013 Org-wide updates  </li> <li><code>Engineering Huddle</code> \u2013 Technical discussions  </li> <li><code>Sikshana Foundation</code>, <code>Khushi Baby</code>, <code>Vision Empower</code> \u2013 Sector/project updates with external teams  </li> </ul> </li> </ol>"},{"location":"onboarding/#3-development-environment-setup","title":"3. \ud83d\udcbb Development Environment Setup","text":"<ol> <li>GitHub Access \u2013 Make sure you\u2019re added to the A4I organization.  </li> <li>Clone Repos \u2013 Start with cloning the repository applicable and try to follow the readme steps to get the pre-requisites and the application working.  </li> <li>Install Required Tools:</li> <li>Node.js LTS</li> <li>Python 3.10+</li> <li>Docker Desktop</li> <li>VS Code (recommended editor)</li> <li>Other specific datastores, libraries and tools as suggested in each repository</li> <li>Environment Variables \u2013 Use <code>.env.example</code> files in repos; request actual secrets from admin (never stored in GitHub).  </li> </ol>"},{"location":"onboarding/#4-a4i-impact-areas","title":"4. \ud83c\udf0d A4I Impact Areas","text":""},{"location":"onboarding/#healthcare-impact","title":"\ud83c\udfe5 Healthcare Impact","text":"<ul> <li>\ud83d\udcd1 ASHABot Launch Presentation </li> <li>\ud83d\udcc4 ASHABot Research Paper </li> <li>\ud83d\udcbb Codebase: byoeb</li> </ul>"},{"location":"onboarding/#education-impact","title":"\ud83d\udcda Education Impact","text":"<ul> <li>\ud83d\udcd1 Shiksha Foundation Launch Presentation </li> <li>\ud83d\udcc4 Shiksha Research Paper </li> <li>\ud83d\udcbb Codebase: Shiksha Copilot </li> <li>\ud83c\udfa5 YouTube Training Videos</li> </ul>"},{"location":"onboarding/#accessibility-impact","title":"\u267f Accessibility Impact","text":"<ul> <li>\ud83d\udcd1 Vision Empower Presentation </li> <li>\ud83d\udcc4 Vision Empower Research Paper </li> <li>\ud83d\udcbb Codebase: SEEDS </li> </ul>"},{"location":"onboarding/#need-help","title":"\ud83e\udd1d Need Help?","text":"<p>Reach out to your assigned buddy if you have any queries or run into issues. In case there is no assigned buddy yet, contact @soumabha or @kaltinhoth .  For general issues, you can also drop an email to <code>a4i@iiitb.ac.in</code></p>"},{"location":"workflows/","title":"\ud83d\udd04 Workflows","text":""},{"location":"workflows/#git-workflow","title":"Git Workflow","text":"<ul> <li>Branch naming: Since a4i does a lot of cross collaboration with different agencies - the branch naming convention starts with the org-name, followed by the individual-name, the type of task (<code>feature/*</code>. <code>bugfix/*</code>) and a short description. Example: <code>a4i/soumabha/feature/auth-changes</code></li> <li>Master or Main Branch: a4i/main is usually the main branch across all the codebase repositories. Direct fastforward merge / modification of commit history is not permitted on this branch and has special protected privilege</li> <li>Contribution: to contribute to changes, a new branch must be created from the latest master (a4i/main) and a pull request is to be used to merge changes.</li> <li>Compliance and Testing: Every pull request requires at least 1 reviewer to sign off to the changes. The requestor is also expected to pass all the necessary github actions workflow like secret-detection, unit-testing, code and security scanning.</li> </ul>"},{"location":"workflows/#issue-tracking","title":"Issue Tracking","text":"<ul> <li>Uses GitHub Issues</li> <li>Planning is via Project board</li> <li>Project Labels: <code>SEEDS</code>, <code>Shiksha-Copilot</code>, <code>ASHABot</code>, <code>General</code> : to indicate area of impact.</li> <li>Labels: <code>bug</code>, <code>enhancement</code>, <code>retro</code>, <code>infra</code>: to indicate quality and volume.</li> </ul>"},{"location":"workflows/#cicd","title":"CI/CD","text":"<ul> <li>GitHub Actions run lint/tests on PR</li> <li>Staging deployment happens via a4i/main branch</li> </ul>"},{"location":"architecture/Inclusive-Architecture/","title":"Inclusive Agentic Architecture Template","text":"<p>Building Inclusive, Modular, and Agentic Platforms for Education, Healthcare, and Accessibility</p>"},{"location":"architecture/Inclusive-Architecture/#1-abstract","title":"1. Abstract","text":"<p>This whitepaper presents an inclusive, agentic architecture designed to enable scalable, modular, and human-centred AI systems across domains such as education, healthcare, and accessibility.</p> <p>Current digital ecosystems are fragmented, leading to duplication, limited reuse, and difficulty in scaling innovations. The proposed framework unifies these efforts through a layered reference architecture \u2014 comprising integration channels, experience orchestration, business logic, agent libraries, and tool adapters under an open and extensible model.</p> <p>The architecture emphasizes inclusivity, ensuring accessibility via multimodal channels such as mobile, feature phones, and assistive devices.  It also embeds security, auditability, and compliance guardrails upfront.</p>"},{"location":"architecture/Inclusive-Architecture/#2-motivation","title":"2. Motivation","text":"<ul> <li>Current solutions are point-specific and lack a unified reference model.  </li> <li>Fragmentation challenge: Similar patterns solved differently \u2192 difficult to reuse, scale, or maintain.  </li> <li>Need for a common architectural template that:</li> <li>Reduces duplication of effort across projects.  </li> <li>Ensures scalability &amp; reliability during usage surges.  </li> <li>Simplifies integration of new datastores and connectors.  </li> <li>Embeds security, auditability, and compliance guardrails upfront.  </li> </ul> <p>Long-term Vision: A community-driven reference architecture that improves developer onboarding, speeds up experimentation, and ensures enterprise-grade resilience.</p>"},{"location":"architecture/Inclusive-Architecture/#3-high-level-layered-architecture","title":"3. High-Level Layered Architecture","text":"<p> Figure 1: High-Level Agentic Architecture Template</p>"},{"location":"architecture/Inclusive-Architecture/#4-components-of-the-architecture","title":"4. Components of the Architecture","text":""},{"location":"architecture/Inclusive-Architecture/#41-integration-layer","title":"4.1 Integration Layer","text":"<p>Serves as the system\u2019s backbone for connectivity and interoperability. It manages authentication, auditing, and compliance across integrations. Main goal: make external data feel internal while maintaining isolation and control.</p>"},{"location":"architecture/Inclusive-Architecture/#channels-for-access","title":"Channels for Access","text":"<p>Supports: - Mobile applications - Feature phone\u2013based telecommunications - Web interfaces - Assistive devices (e.g., Hexis, Iris) - Messaging interfaces (WhatsApp, Telegram, SMS)</p>"},{"location":"architecture/Inclusive-Architecture/#data-flow-example","title":"Data Flow Example","text":"<pre><code>sequenceDiagram\n    participant User\n    participant ChannelAdapter\n    participant IntegrationLayer\n    participant ExperienceLayer\n\n    User-&gt;&gt;ChannelAdapter: Message / Event\n    ChannelAdapter-&gt;&gt;IntegrationLayer: Normalized request\n    IntegrationLayer-&gt;&gt;ExperienceLayer: Authenticated, enriched payload</code></pre>"},{"location":"architecture/Inclusive-Architecture/#experience-layer-with-user-conversation-and-memory","title":"Experience Layer (with User, Conversation and Memory)","text":"<p>Acts as the central user interaction entry point, normalizing inputs from diverse channels for unified downstream handling. It enriches the data with context, authentication, and personalization cues, and formats output for the appropriate channel.</p>"},{"location":"architecture/Inclusive-Architecture/#example-data-flow","title":"Example Data Flow","text":""},{"location":"architecture/Inclusive-Architecture/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant User\n    participant IntegrationLayer\n    participant ExperienceLayer\n    participant BusinessOrchestration\n\n    User-&gt;&gt;IntegrationLayer: Sends message via channel adapter &lt;br/&gt;(text, audio, button click)\n    IntegrationLayer-&gt;&gt;ExperienceLayer: Enriched, authenticated input\n    Note right of ExperienceLayer: Create &lt;br/&gt;`standard_request` JSON &lt;br/&gt;with metadata:&lt;br/&gt;\u2022 channel_type&lt;br/&gt;\u2022 user_id&lt;br/&gt;\u2022 locale&lt;br/&gt;\u2022 timestamp&lt;br/&gt;\u2022 raw_content\n    ExperienceLayer-&gt;&gt;BusinessOrchestration: Forward structured request&lt;br/&gt;via REST or Message Bus</code></pre> <p>Sample json <pre><code>{\n  \"standard_request\": {\n    \"channel_type\": \"whatsapp\",\n    \"user_id\": \"ASHA_908\",\n    \"locale\": \"bn\",\n    \"timestamp\": \"2025-10-08T08:20:30Z\",\n    \"content\": \"What are the symptoms of dengue?\"\n  }\n}\n</code></pre></p>"},{"location":"architecture/Inclusive-Architecture/#example-memory-query","title":"Example Memory Query","text":"<pre><code>{\n  \"user_id\": \"ASHA_908\",\n  \"context_type\": \"conversation_summary\"\n}\n</code></pre> <p>Response: <pre><code>{\n  \"last_session_date\": \"2025-10-01\",\n  \"topics_discussed\": [\"child vaccination\", \"vaccine application\"],\n  \"pending_action\": \"confirm new date\",\n  \"raw_messages\": [{\n       ...\n       \"source\": \"whatsapp\",\n       ...\n   }]\n}\n</code></pre></p> <p>The Experience Layer forwards this normalized structure to the Business Orchestration Layer.</p>"},{"location":"architecture/Inclusive-Architecture/#42-business-agentic-orchestration-layer","title":"4.2 Business &amp; Agentic Orchestration Layer","text":"<p>Implements domain-level logic through a hybrid of deterministic services and agentic orchestration.</p> <ul> <li>Business Services: Integrate user information, manage persistent data, and handle workflows.  </li> <li>Agent-Oriented Orchestration: Delegates reasoning and transformation tasks to agents.</li> </ul> <p>In effect, it acts as the \u201cbrain\u201d for domain behaviour, handling branching logic, fallback, error handling, and domain-specific rules or constraints.</p>"},{"location":"architecture/Inclusive-Architecture/#agent-library","title":"Agent Library","text":"<p>Agents operate as autonomous reasoning nodes that receive and emit structured events.  Each agent processes data and can be chained in different order enabling concurrent reasoning and flexible delegation without direct coupling.</p> <p>Encapsulates AI reasoning and cognitive tasks into reusable \"agents\", allowing complex system logic to be broken into small, composable and reusable \"units of work\".</p> Agent Type Function Example Use Case RAG Retrieval-Augmented Generation Query knowledge base Translation Multilingual Interaction Local language conversion ASR/TTS Speech Processing IVR / audio channels Consensus Evaluator Group agreement logic Feedback loops Data Sanity Validation and safety Input consistency <p>Each agent defines input/output contracts and performance expectations for interoperability.</p> <p>Lifecycle and Agent Invocation</p> <p>Step 1: Registered in the central Agent Registry under a versioned identifier.</p> <p>Step 2: Invoked by an Orchestrator or another agent based on workflow logic.</p> <p>Step 3: Executed with full context (user/session/memory data).</p> <p>Step 4: Returns standardized output for downstream agents or final rendering.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant Orchestrator\n    participant AgentRegistry\n    participant RAGAgent\n    participant TTSAgent\n\n    Orchestrator-&gt;&gt;AgentRegistry: Lookup(\"rag_agent:v1\")\n    AgentRegistry--&gt;&gt;Orchestrator: Return config &amp; schema\n    Orchestrator-&gt;&gt;RAGAgent: Execute(query=\"symptoms of dengue\")\n    RAGAgent--&gt;&gt;Orchestrator: Return summary + confidence\n    Orchestrator-&gt;&gt;TTSAgent: Convert text \u2192 speech\n    TTSAgent--&gt;&gt;Orchestrator: Return audio file reference\n</code></pre>"},{"location":"architecture/Inclusive-Architecture/#tool-library","title":"Tool Library","text":"<p>Tools are deterministic connectors or adapters that perform actions or I/O without reasoning.</p> Tool Purpose Example WhatsApp Adapter Messaging Send notifications IVR Gateway Telephony Play prompts Email Tool Communication Send updates CRM Adapter External Systems Update lead data MongoDB Adapter Storage Persist structured data <pre><code>sequenceDiagram\n    autonumber\n    participant Agent\n    participant ToolRegistry\n    participant WhatsAppAdapter\n    participant ExternalAPI\n\n    Agent-&gt;&gt;ToolRegistry: Lookup(\"whatsapp_adapter:v2\")\n    ToolRegistry--&gt;&gt;Agent: Return schema + endpoint\n    Agent-&gt;&gt;WhatsAppAdapter: Send(recipient=\"+91XXXX\", template=\"notify_schedule\")\n    WhatsAppAdapter-&gt;&gt;ExternalAPI: POST /messages\n    ExternalAPI--&gt;&gt;WhatsAppAdapter: {status: \"success\", id: \"wa_001\"}\n    WhatsAppAdapter--&gt;&gt;Agent: status=success, timestamp</code></pre>"},{"location":"architecture/Inclusive-Architecture/#example-tool-invocation","title":"Example Tool Invocation","text":"<pre><code>{\n  \"tool\": \"whatsapp_adapter\",\n  \"action\": \"send_message\",\n  \"recipient\": \"+91XXXXXXXXXX\",\n  \"message\": \"Your vaccination is scheduled for 12 Oct at 10 AM.\",\n  \"template_id\": \"notify_schedule\"\n}\n</code></pre> <p>Real World Example: Sahayak Orchestrator</p> <p>The Sahayak Orchestrator exemplifies how these architectural layers operate in unison.  - It receives user inputs\u2014along with historical context and session memory\u2014and determines which agents or tools should act next.  - Using its centralized Tool Registry, populated with system-level MCP (Multi-Channel Plugin) abstractions, the orchestrator dynamically routes requests to the appropriate agents for reasoning and to the corresponding tools for deterministic execution.  - This event-driven workflow enables Sahayak to coordinate human-like reasoning with system-level precision, ensuring adaptive, multimodal interactions across channels and services.</p> <pre><code>sequenceDiagram\n    autonumber\n    participant ExperienceLayer\n    participant Orchestrator\n    participant Agent\n    participant Tool\n    participant ExternalService\n\n    ExperienceLayer-&gt;&gt;Orchestrator: User prompt, context or event data\n    Orchestrator-&gt;&gt;Agent: SahayakTextRequestEvent\n    Agent-&gt;&gt;Tool: ToolCallRequestEvent (resolved via known_tool_owners)\n    Tool-&gt;&gt;ExternalService: Executes operation (e.g., send message)\n    ExternalService--&gt;&gt;Tool: Returns API result\n    Tool--&gt;&gt;Agent: ToolCallExecutionEvent (structured response)\n    Agent--&gt;&gt;Orchestrator: Final message event\n    Orchestrator--&gt;&gt;ExperienceLayer: Rendered response</code></pre>"},{"location":"architecture/Inclusive-Architecture/#43-ingestion-layer","title":"4.3 Ingestion Layer","text":"<p>converts raw or semi-structured content (textbooks, media, logs) into enriched, searchable knowledge representations for downstream agents. Supports high-throughput ingestion, context caching, and long-term knowledge storage.</p>"},{"location":"architecture/Inclusive-Architecture/#data-flow_1","title":"Data Flow","text":"<pre><code>flowchart LR\n    A[1\ufe0f\u20e3 Ingestion Trigger] --&gt; B[2\ufe0f\u20e3 Data Retrieval]\n    B --&gt; C[3\ufe0f\u20e3 Parsing &amp; Preprocessing]\n    C --&gt; D[4\ufe0f\u20e3 Chunking &amp; Metadata Enrichment]\n    D --&gt; E[5\ufe0f\u20e3 Embedding &amp; Indexing]\n    E --&gt; F[6\ufe0f\u20e3 Storage &amp; Caching]\n    F --&gt; G[7\ufe0f\u20e3 Event Logging &amp; Completion]</code></pre> <p>Ingestion layer relies heavily on the agent and tool libraries preprovisioned. Sample examples</p> Type Responsibility Example Components Parser Agent Handles parsing, cleaning, extraction logic. <code>TextExtractionAgent</code>, <code>SpeechToTextAgent</code> Chunking Agent Segments and structures data. <code>ChunkingAgent</code>, <code>ContentSplitterTool</code> Embedding Agent Generates semantic embeddings from processed text. <code>EmbeddingAgent</code>, <code>OpenAIEmbeddingTool</code> Storage / Memory Tools Persist processed data into memory or vector DBs. <code>VectorDBTool</code>, <code>KnowledgeBaseAdapter</code>, <code>CacheTool</code> Logging &amp; Audit Agents Maintain ingestion logs, telemetry, and versioned lineage. <code>IngestionLoggerAgent</code>, <code>TelemetryCollectorTool</code>"},{"location":"architecture/Inclusive-Architecture/#5-real-world-applications","title":"5. Real-World Applications","text":""},{"location":"architecture/Inclusive-Architecture/#education","title":"Education","text":"<ul> <li>Teachers can aggregate lesson plans, generate evaluations, and access learning content through multilingual and multimodal channels.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#healthcare","title":"Healthcare","text":"<ul> <li>Field health workers (ASHAs/ANMs) can report via WhatsApp or IVR, enabling data capture and alerts in remote areas.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#accessibility","title":"Accessibility","text":"<ul> <li>Voice and assistive-device-based interfaces ensure participation for users with disabilities or low-tech environments.</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#6-design-principles","title":"6. \ud83c\udfaf Design Principles","text":"\ud83e\udde9 Principle \ud83d\udca1 Description \ud83d\udd17 Implication \ud83c\udf10 Open Source First Built to evolve collaboratively and transparently. Encourages community contributions and shared innovation. \ud83e\uddf1 Highly Modular Each capability functions as a reusable component or library. Simplifies updates, enables team-specific extensions. \u2696\ufe0f Horizontally Scalable Designed for distributed, cloud-native scaling. Supports large concurrent workloads and adaptive scaling. \u267f Inclusive &amp; Multimodal Accessible via text, voice, and assistive devices. Ensures equitable access across digital divides. \ud83e\udd16 AI-First Agentic reasoning and cognitive workflows by design. Enables contextual automation and hybrid human-AI work. \ud83d\udd12 Secure by Design Data flow and access control built into architecture. Simplifies compliance, auditability, and data governance. \ud83d\udd0c Plug &amp; Play Extensible via standardized connectors and APIs. Allows rapid onboarding of new channels or services. <p>\ud83d\udcac These principles form the DNA of the Agentic Template \u2014 ensuring it stays modular, scalable, and inclusive across domains.</p>"},{"location":"architecture/Inclusive-Architecture/#7-limitations-future-considerations","title":"7. Limitations &amp; Future Considerations","text":"<ul> <li>Complex Orchestration: Debugging multi-agent workflows may be non-trivial.  </li> <li>Connectivity Dependency: Cloud-dependent features (RAG, ASR/TTS) may face offline limitations.  </li> <li>Tool Explosion: Too many tools risk hallucinated abilities. Guardrails and observability layers are essential.  </li> <li>Maintenance Overhead: Hybrid agentic-service systems require continuous synchronization and version management.</li> <li>Evolving Standards: Interoperability may drift with external APIs and / or AI model changes</li> </ul>"},{"location":"architecture/Inclusive-Architecture/#8-references","title":"8. References","text":"<ul> <li>Inclusive Architecture Whitepaper (2025) </li> <li>Architecture Template (Draft, A4I, Aug 2025) </li> <li>Microsoft Azure AI &amp; Cognitive Services Documentation  </li> <li>OpenAI Function Calling &amp; Orchestration Patterns (2024)  </li> <li>WHO Digital Health Guidelines (2023)</li> </ul> <p>\u00a9 2025 A4I. All Rights Reserved.</p>"},{"location":"architecture/universal-knowledge-ingestion/","title":"Universal Knowledge Ingestion Framework","text":""},{"location":"architecture/universal-knowledge-ingestion/#purpose-and-scope","title":"Purpose and Scope","text":"<p>This document proposes a common, inclusive knowledge ingestion framework that can be used for digital public AI good: - BYOEB (ASHA WhatsApp RAG for healthcare) - Shiksha Copilot (lesson outcome\u2013driven education copilot) - SEEDS (audio-first IVR and conferencing for visually impaired learners)</p> <p>\u2026and is extensible to new products and modalities.</p> <p>It aligns with the Inclusive Architecture\u2019s Ingestion Layer and its agent/tool model \u2014 \u201cingestion trigger \u2192 retrieval \u2192 parsing \u2192 chunking \u2192 embedding \u2192 storage \u2192 logging\u201d \u2014 and implements those as reusable agents and tools across domains.</p>"},{"location":"architecture/universal-knowledge-ingestion/#layered-architecture","title":"Layered Architecture","text":""},{"location":"architecture/universal-knowledge-ingestion/#design-drivers","title":"Design drivers","text":"<ol> <li> <p>Aligned with Inclusive Architecture</p> <ul> <li>Ingestion is a layer serving multiple domains (education, health, accessibility) via a common set of agents &amp; tools.</li> <li>All products talk to ingestion through consistent APIs/events; ingestion itself is multi-tenant and domain-aware.</li> </ul> </li> <li> <p>Multi\u2011Modal</p> <ul> <li>Documents &amp; text: PDFs, DOCX, plain text, HTML, FAQs.</li> <li>Audio: IVR recordings, TTS outputs, podcasts/YouTube, etc.</li> <li>Future: images, structured data (CSV, logs), etc.</li> </ul> </li> <li> <p>Multi\u2011Model &amp; Multi\u2011Store</p> <ul> <li>Plug-and-play LLM providers (Azure OpenAI, OpenAI, OpenRouter, local models via Ollama).</li> <li>OpenRouter as a default unified gateway: Access 100+ models (Claude, Gemini, Llama, Mistral) through a single API, enabling cost optimization and model experimentation without provider lock-in.</li> <li>Pluggable vector stores (Azure Vector Search, Qdrant, Chroma, pgvector\u2026).</li> <li>Other stores: blob/file storage, relational/NoSQL metadata, graph DB (Neo4j), and search indexes.</li> </ul> </li> <li> <p>Domain-Aware but Reusable</p> <ul> <li>Shiksha\u2019s LO extraction and curriculum KG should be profiles on top of generic steps, not one-off pipelines.</li> <li>BYOEB\u2019s KB ingestion should be the default text ingestion profile.</li> <li>SEEDS\u2019 audio/transcription/knowledge items flow should be the default audio profile.</li> </ul> </li> <li> <p>Inclusive &amp; Operational</p> <ul> <li>Support low-resource environments (local vector stores, offline-first when possible).</li> <li>Strong observability, versioning, audit, and safety/quality checks.</li> </ul> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#conceptual-model","title":"Conceptual Model","text":"<p>We define three layers inside the ingestion system (itself mapped to the \u201cIngestion Layer\u201d in the Inclusive Architecture): </p> <ol> <li>Source &amp; Trigger Layer: Where content comes from and why we ingest it.</li> <li>Processing Pipeline Layer (Agents &amp; Tools): How content is parsed, cleaned, enriched, and encoded as knowledge.</li> <li>Storage &amp; Registration Layer: Where the knowledge representations live and how they are registered for use by RAG/agents.</li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#canonical-knowledge-item","title":"Canonical \"Knowledge Item\"","text":"<p>All pipelines ultimately produce a canonical <code>KnowledgeItem</code> (concept, not implementation-bound) with:</p> Field Description <code>id</code> UUID <code>tenant_id</code> Tenant/partner/org context <code>domain_profile</code> education_shiksha / health_byoeb / accessibility_seeds / etc. <code>modality</code> <code>text</code>, <code>document</code>, <code>audio</code>, <code>mixed</code> <code>source_uri</code> Blob URI / URL / IVR recording ID <code>metadata</code> Board/grade/subject/chapter, language, tags, timestamps <code>structured_views</code> LO trees, KG nodes, transcript segments <code>embeddings</code> Vector store pointers (no raw vectors in Postgres) <code>lineage</code> Pipeline version, model version, ingestion time <p>BYOEB\u2019s <code>TextNode</code> + vector metadata, Shiksha Copilot's <code>StepResult</code> and SEEDS\u2019 <code>KnowledgeItem</code> schema can be mapped into this conceptual model.</p>"},{"location":"architecture/universal-knowledge-ingestion/#high-level-ingestion-flow","title":"High-Level Ingestion Flow","text":"<p>We define a generic pipeline that all domain profiles reuse:</p> <ol> <li>Ingestion Trigger</li> <li>Data Retrieval</li> <li>Parsing &amp; Preprocessing</li> <li>Chunking &amp; Metadata Enrichment</li> <li>Embedding &amp; Indexing</li> <li>Storage &amp; Caching</li> <li>Event Logging &amp; Completion</li> </ol> <p>We implement each step as one or more agents (reasoning/LLM heavy) and tools (IO or deterministic work).</p>"},{"location":"architecture/universal-knowledge-ingestion/#core-implementation","title":"Core Implementation","text":""},{"location":"architecture/universal-knowledge-ingestion/#source-and-trigger-layer","title":"Source And Trigger Layer","text":"<p>Triggers:  - API calls (e.g., <code>/vector/index</code> in BYOEB) - Webhook/events: \u201cfile uploaded to blob\u201d, \u201cnew textbook version published\u201d, \u201caudio recording completed from IVR\u201d - Scheduled jobs: \u201csync YouTube playlist weekly\u201d, \u201cre-ingest content when embedding model changes\u201d,  - Manual CLI/ops triggers</p> <p>Source Connectors: A standard source connector would implement the interface <pre><code>interface SourceConnector {\n  listResources(filter): Promise&lt;SourceResource[]&gt;;\n  fetch(resourceId): Promise&lt;RawArtifact&gt;;  // bytes + metadata\n}\n</code></pre></p> <p>Examples: - AzureBlobConnector \u2014 already exists in BYOEB. - HttpUrlConnector \u2014 general web pages / PDFs. - ExternalMediaConnector \u2014 YouTube, podcasts, RSS, S3/GCS, etc., as in SEEDS\u2019s ExternalSource model and audioDownloaderService. - IVRRecordingConnector \u2014 SEEDS\u2019 IVR recordings; uses IVR backend IDs.</p>"},{"location":"architecture/universal-knowledge-ingestion/#processing-pipeline-layer-agents-tools","title":"Processing Pipeline Layer (Agents &amp; Tools)","text":"<p>We define a Pipeline Orchestrator that runs a graph of steps, where each step is an <code>IngestionAgent</code> or/and <code>IngestionTool</code> with explicit input/output contracts. This reuses the agent &amp; tool library philosophy from the Inclusive Architecture.</p> <p>Common Agents and Tool types 1. Parser Agents:     - <code>DocumentParserAgent</code>: (PDF, DOCX \u2192 markdown/structured JSON)       - Implements Shiksha\u2019s TOC discovery, splitting, and LLM/vision-based extraction as sub-steps.       - Supports alternative engines (MinerU, OCR).    - <code>TextParserAgent</code>: (plain text, HTML, FAQs \u2192 normalized text). Generalizes BYOEB\u2019s LlamaIndex text parsing.    - <code>AudioParserAgent</code> (audio \u2192 transcription + timestamps). Wraps SEEDS\u2019 TranscriptionService (Azure STT).</p> <ol> <li>Cleaning &amp; Normalization Agents:</li> <li><code>MarkdownCleaningAgent</code> \u2014 Shiksha\u2019s TextCleaningStep (LLM-powered cleaning) generalized to all markdown sources.</li> <li> <p><code>TranscriptionPostProcessorAgent</code> \u2014 punctuation, diarization, noise trimming, language detection for audio transcripts.</p> </li> <li> <p>Enrichment &amp; Domain Agents</p> </li> <li><code>LearningOutcomeExtractionAgent</code> \u2014 Shiksha LO extraction steps (chapter-level, subtopic-level) exposed as a reusable agent that works on any curriculum-like content.</li> <li><code>KnowledgeGraphBuilderAgent</code> \u2014 uses Shiksha KG steps (entities, relationships, Neo4j export).</li> <li><code>FAQGenerationAgent</code> \u2014 optional; generate FAQs for BYOEB or SEEDS from long docs/audio summaries.</li> <li> <p><code>KeywordAndTaggingAgent</code> \u2014 used in SEEDS to derive keywords/tags; can be reused elsewhere.</p> </li> <li> <p>Chunking Agents</p> </li> <li><code>SentenceChunkingAgent</code> \u2014 wraps BYOEB's LlamaIndex SentenceSplitter with configurable chunk_size / overlap.</li> <li><code>PageChunkingAgent</code> \u2014 Shiksha's page-based splitting for textbooks.</li> <li> <p><code>TimedChunkingAgent</code> \u2014 for audio segments (based on transcript timestamps) to create semantically and temporally aligned chunks for SEEDS.</p> <p>How Time-Based Chunking Works:  1. The <code>AudioParserAgent</code> produces a transcript with word-level or segment-level timestamps (e.g., from Azure STT's <code>word_timings</code>).  2. <code>TimedChunkingAgent</code> receives this timestamped transcript and applies configurable rules:     - Fixed duration windows: Split every N seconds (e.g., 30s chunks) with optional overlap.     - Silence/pause detection: Use natural pauses (&gt;500ms) as chunk boundaries.     - Semantic + temporal hybrid: Combine sentence boundaries with time limits \u2014 prefer splitting at sentence ends, but force-split if duration exceeds max (e.g., 60s).  3. Each resulting chunk carries metadata: <code>{ \"start_time\": 45.2, \"end_time\": 72.8, \"duration\": 27.6 }</code>.  4. These timestamps enable audio seek during playback in SEEDS IVR/teacher experiences \u2014 users can jump to the relevant audio segment for any retrieved chunk.</p> </li> <li> <p>Embedding &amp; Indexing Agents</p> </li> <li> <p><code>EmbeddingAgent</code> \u2014 wraps any embedding model; configuration chooses:</p> <ul> <li>Provider: Azure OpenAI (current), OpenAI API, local model, etc.</li> <li>Model: <code>text-embedding-3-large</code>, others; embedding dimension recorded in metadata.</li> <li><code>VectorIndexingAgent</code> \u2014 writes to pluggable vector stores.</li> <li><code>GraphIndexingAgent</code> \u2014 writes to Neo4j (for curriculum KG).</li> <li><code>KeywordIndexingAgent</code> \u2014 writes full-text indexes (e.g., Mongo text indexes used in SEEDS).</li> </ul> </li> <li> <p>Governance &amp; Logging Agents</p> <ul> <li><code>IngestionLoggerAgent</code> \u2014 logs events, durations, outcomes (success/fail).</li> <li><code>DataLineageAgent</code> \u2014 persistent record with full traceability of the chain including sources, pipeline and versions, LLMs, vector store, etc. This enables reproducability, impact analysis, audit compliance and debugging.      </li> <li><code>QualityVerifierAgent</code> \u2014 sample-based QA (e.g., compare extracted text with original pages; check LO consistency).</li> </ul> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-orchestration","title":"Pipeline Orchestration","text":"<p>Static Orchestration with LLM-Powered Steps:</p> <p>The ingestion framework uses declarative, static pipelines \u2014 NOT LLM-driven orchestration. This is a deliberate design choice:</p> Aspect Our Approach Alternative (LLM Orchestration) Step sequencing Defined in YAML, runs sequentially LLM decides next step dynamically LLM usage Within individual steps (parsing, cleaning, extraction) Also for routing/decision-making Predictability High \u2014 same input produces same flow Variable \u2014 LLM may choose different paths Cost Lower \u2014 LLM calls only where needed Higher \u2014 additional reasoning calls Debuggability Easy \u2014 trace through known steps Harder \u2014 must understand LLM's decisions Auditability Full \u2014 pipeline version = exact behavior Partial \u2014 depends on LLM reasoning logs <p>Why static orchestration? - Ingestion is a batch/background process where predictability and reproducibility matter more than flexibility. - Each step already uses LLMs where valuable (vision-based parsing, text cleaning, LO extraction). - Pipeline versioning ensures identical behavior across runs \u2014 critical for compliance and debugging. - Cost-efficient: No \"reasoning overhead\" for routing decisions.</p> <p>Example: YAML, JSON of a pipeline described as declarative configs <pre><code>pipeline_id: shiksha_textbook_v1\ndomain_profile: education_shiksha\ninput_modality: document\nsteps:\n  - agent: DocumentParserAgent\n    config: { strategy: \"llm_vision\" }\n  - agent: MarkdownCleaningAgent\n  - agent: LearningOutcomeExtractionAgent\n  - agent: SentenceChunkingAgent\n    config: { chunk_size: 1024, overlap: 100 }\n  - agent: EmbeddingAgent\n    config: { model: \"text-embedding-3-large\", provider: \"azure\" }\n  - agent: VectorIndexingAgent\n    config: { store: \"qdrant\" }\n  - agent: KnowledgeGraphBuilderAgent\n    config: { graph_store: \"neo4j\" }\n</code></pre></p> <p>BYOEB\u2019s current <code>KBService.upload()</code> orchestration (download \u2192 parse \u2192 similarity check \u2192 add to vector store) becomes one such pipeline (health_kb_v1), and its BaseVectorStore abstraction can be moved into the shared library.</p> <p>SEEDS\u2019 \u201caudio \u2192 transcription \u2192 keywords \u2192 embedding \u2192 KnowledgeItem\u201d flow becomes <code>accessibility_audio_v1</code>.</p>"},{"location":"architecture/universal-knowledge-ingestion/#storage-registration-layer","title":"Storage &amp; Registration Layer","text":"<p>Storage Layer 1. Object Storage  <pre><code>interface ObjectStore {\n  put(bytes, key, metadata?): Promise&lt;string&gt;;  // returns URI\n  get(key): Promise&lt;RawArtifact&gt;;\n}\n</code></pre>   - Implementation: Azure Blob (existing in BYOEB &amp; SEEDS).   - Used for PDF chapters, processed markdown, audio files, etc.</p> <ol> <li>Vector Store <pre><code>interface VectorStore {\n  upsert(vectors: VectorRecord[]): Promise&lt;void&gt;;\n  search(query_vector, k, filters?): Promise&lt;VectorRecord[]&gt;;\n  count(): Promise&lt;number&gt;;\n}\n</code></pre></li> <li>Backends:<ul> <li>Azure Vector Search (BYOEB).</li> <li>Qdrant (Shiksha).</li> <li>Chroma / pgvector for local/offline scenarios.</li> </ul> </li> <li> <p>Reuse BYOEB\u2019s BaseVectorStore design and vector upsert/similarity logic (including 0.95 similarity de-duplication).</p> </li> <li> <p>Graph Store <pre><code>interface GraphStore {\n  upsertNodes(nodes: Node[]);\n  upsertEdges(edges: Edge[]);\n}\n</code></pre></p> </li> <li> <p>Neo4j backing for Shiksha\u2019s KG export.</p> </li> <li> <p>Metadata Store</p> </li> <li>Prefer a common metadata/entity store (e.g., Mongo/Postgres) that holds <code>KnowledgeItem</code> docs referencing object/vector/graph layers.</li> <li> <p>SEEDS\u2019 <code>KnowledgeItem</code> model is a good starting point, extended slightly to match the canonical KnowledgeItem spec.</p> </li> <li> <p>Registration and Discovery</p> </li> <li>After ingestion, each pipeline registers a <code>KnowledgeItem</code> (or batch) into a Knowledge Registry:</li> <li>Saves core attributes (domain_profile, modality, metadata).</li> <li>Records pointers to:<ul> <li>Blob URIs for raw/processed files.</li> <li>Vector store IDs / collections.</li> <li>Graph DB node IDs.</li> </ul> </li> <li>Agents (e.g., RAG agents in Experience Layer) ask a KnowledgeBaseAdapter tool for:<ul> <li><code>get_items(domain_profile, filters)</code></li> <li><code>semantic_search(query, profile, k, filters)</code></li> <li><code>get_by_id(id)</code></li> </ul> </li> </ol> <p>This is the single consistent entrypoint for BYOEB chat, Shiksha copilot, and SEEDS IVR/teacher experiences to reuse the same knowledge.</p>"},{"location":"architecture/universal-knowledge-ingestion/#domain-profiles","title":"Domain Profiles","text":"<p>Domain profiles use YAML schema with multi tenant isolation &amp; secrets</p> <ol> <li> <p>YAML: Public profiles live in git and describe what to do, not how to authenticate.</p> </li> <li> <p>Secretful Config (private): A separate, non-public config (env or private YAML) maps these logical refs to real credentials.</p> </li> </ol>"},{"location":"architecture/universal-knowledge-ingestion/#multi-tenant-support","title":"Multi-Tenant Support","text":"<p>Every pipeline run is created with a tenant. The runner uses: - <code>tenant.slug</code> plus template strings in YAML to produce:     - Vector index name / collection.     - Graph DB database/namespace. - Postgres tables always carry tenant_id, so queries are scoped.</p> <p>That gives: - Logical separation in all stores. - Ability to run a single profile across many tenants.</p> <p>Tenant\u2013Pipeline Relationship:</p> <p>A single tenant can use multiple pipelines simultaneously. For example, a state education board tenant (<code>kseeb_grade6</code>) might use: - <code>shiksha_textbook_v1</code> for textbook ingestion - <code>shiksha_audio_v1</code> for supplementary audio content - <code>generic_faq_v1</code> for FAQ documents</p> <p>The <code>pipeline_runs</code> table links each run to both <code>tenant_id</code> and <code>pipeline_version_id</code>, so: - Tenants can mix-and-match pipelines as needed. - Each pipeline can be versioned independently. - Retrieval queries can filter by <code>domain_profile</code> or span across all knowledge for that tenant.</p> <p>Pipeline YAML configs are versioned in git (e.g., <code>profiles/shiksha_textbook_v1.yaml</code>). The <code>pipeline_versions</code> table's <code>yaml_profile_name</code> field references this file. When a pipeline evolves, we create a new version (v1.1, v2.0) rather than modifying the existing YAML, ensuring reproducibility and auditability.</p>"},{"location":"architecture/universal-knowledge-ingestion/#how-this-hooks-back-into-the-orchestration-layer","title":"How This Hooks Back into the Orchestration Layer","text":"<p>From the Orchestrator\u2019s point of view, Ingestion is just another tool: - Tool: <code>ingest_content</code>     - Inputs: { <code>tenant_slug</code>, <code>pipeline_name</code>, <code>pipeline_version?</code> , <code>source_uri</code>, <code>extra_params</code> }     - Output: { <code>pipeline_run_id</code>, <code>knowledge_item_ids[]</code> } - Tool: <code>search_knowledge</code>     - Inputs: { <code>tenant_slug</code>, <code>domain_profile</code>, <code>query</code>, <code>filters</code> }      - Uses Postgres + configured vector store to:        - pick the right indexes (<code>vector_store_space</code>)        - call vector search        - hydrate results from <code>chunks</code> and <code>knowledge_items</code>.</p> <p>This gives one Knowledge Base interface for all three systems &amp; any new ones.</p>"},{"location":"architecture/universal-knowledge-ingestion/#backward-compatibility-versioning-strategy","title":"Backward Compatibility &amp; Versioning Strategy","text":""},{"location":"architecture/universal-knowledge-ingestion/#register-current-workflows-as-v0x-pipelines","title":"Register Current Workflows as v0.x Pipelines","text":"<p>BYOEB - Treat the current <code>KBService.upload()</code> + Azure Blob download as pipeline byoeb_kb version 0.1. - The FastAPI endpoint <code>/vector/index</code> continues to call kb_upload(). Internally, we:     - Call the new PipelineRunner with <code>pipeline_version='0.1'</code>.     - Use the <code>BYOEBKBUploadStep</code> adapter which mostly just wraps existing behaviour.     - Pipe logs into Postgres <code>pipeline_runs</code> / <code>pipeline_step_runs</code> as well as existing logs.</p> <p>Shiksha Copilot - The current <code>pipeline_runner.py</code> config is effectively pipeline <code>shiksha_textbook</code> version <code>0.1</code>. - We wrap its steps to conform to the new Step protocol and run it under the Ingestion Service for new ingestions while still allowing CLI-style runs.</p> <p>SEEDS No live code yet, so first implementation can start directly as seeds_audio version 1.0.</p>"},{"location":"architecture/universal-knowledge-ingestion/#introducing-v1x-pipelines","title":"Introducing v1.x Pipelines","text":"<p>For each domain we can then evolve: byoeb_kb v1.0: Maybe adds translation agent, or uses a different parser.In new framework:</p> <p>Connectors - AzureBlobConnector using existing async blob client.</p> <p>Steps - BYOEBListFilesStep (optional) \u2192 lists &amp; filters FileMetadata. - BYOEBDownloadStep wrapping _abulk_download_files. - BYOEBParseChunksStep wrapping text_parser.get_chunks_from_collection(...). - BYOEBUpsertStep wrapping _gather_similar_chunks + _add_nodes_to_vector_store.</p> <p>shiksha_textbook v1.0: Moves to Postgres-backed registry; uses improved chunking for better RAG. In new framework:</p> <p>Each of Steps 3\u20138 (text extraction, cleaning, LO extraction, subtopic cleaning, subtopic-wise LO extraction, create index) becomes a <code>Step</code> in the Ingestion Service.</p> <p>The <code>CreateIndexStep</code> additionally: - Writes knowledge_items row for the chapter, with metadata <code>{board, grade, subject, chapter_number, medium}</code>. - Writes <code>chunks</code> rows for each page/subtopic chunk with Qdrant pointers.</p> <p>Optional KG steps become a separate pipeline (shiksha_kg_v1) that reads from the same cleaned markdown and writes to the graph store.</p> <p>The Shiksha \u201cexperience layer\u201d agents then call the KnowledgeBase tool using board/grade/subject or a combination of all 3 filters that are now available across all stores and Postgres.</p> <p>seeds_audio v1.0: First implementation. Because each <code>knowledge_item</code> row carries pipeline_version_id, retrieval logic can behave differently if necessary (e.g., interpret metadata shapes slightly differently). In new framework:</p> <ul> <li><code>ExternalSource</code> + <code>audioDownloaderService</code> \u2192 <code>SourceConnector</code>implementations.</li> <li>Transcription and keyword extraction steps \u2192 agents in the pipeline.</li> <li>KnowledgeItem DB model becomes a tenant-specific view of the generic <code>knowledge_items</code> table (we can keep a Mongo version or migrate to Postgres gradually).</li> </ul> <p>The canonical <code>knowledge_items</code> row for an audio piece looks like: <pre><code>{\n  \"domain_profile\": \"accessibility_seeds\",\n  \"modality\": \"audio\",\n  \"source_uri\": \"blob://seeds/audio/123.mp3\",\n  \"metadata\": {\n    \"language\": \"kn-IN\",\n    \"theme\": \"math lesson\",\n    \"type\": \"story\",\n    \"duration\": 300,\n    \"tags\": [\"grade6\", \"fractions\"],\n    \"transcript_available\": true\n  }\n}\n</code></pre></p> <p>And chunk rows correspond to transcript segments, with timestamps in <code>metadata</code>.</p> <p>generic_document v1.0: A starter profile for new applications onboarding onto the ingestion framework.</p> <p>This profile provides sensible defaults that work for most text/document use cases without requiring domain-specific customization:</p> <pre><code>pipeline_id: generic_document_v1\ndomain_profile: generic\ninput_modality: document\ndescription: \"Default pipeline for new applications - handles PDFs, DOCX, text files\"\nsteps:\n  - agent: DocumentParserAgent\n    config: \n      strategy: \"auto\"  # auto-detect: use OCR for scanned, direct extraction for digital\n      fallback_to_ocr: true\n  - agent: MarkdownCleaningAgent\n    config:\n      preserve_tables: true\n      preserve_lists: true\n  - agent: SentenceChunkingAgent\n    config: \n      chunk_size: 512\n      overlap: 50\n  - agent: EmbeddingAgent\n    config: \n      model: \"text-embedding-3-small\"  # cost-effective default\n      provider: \"azure\"\n  - agent: VectorIndexingAgent\n    config: \n      store: \"qdrant\"  # or \"${tenant.default_vector_store}\"\n</code></pre> <p>New applications can: 1. Start with generic_document_v1 to get basic ingestion working immediately. 2. Fork and customize \u2014 copy the YAML, add domain-specific agents (e.g., <code>FAQGenerationAgent</code>), adjust chunking parameters. 3. Graduate to a named profile \u2014 once stable, register as <code>myapp_kb_v1</code> with domain_profile <code>myapp</code>.</p> <p>This lowers the barrier to onboarding while maintaining the full power of the framework for teams ready to customize.</p>"},{"location":"architecture/universal-knowledge-ingestion/#low-level-design","title":"Low Level Design","text":""},{"location":"architecture/universal-knowledge-ingestion/#postgres-metadata-registry-design","title":"Postgres Metadata &amp; Registry Design","text":"<p>We\u2019ll put the \u201cbrain\u201d of ingestion metadata in Postgres and keep vectors in the vector stores.</p>"},{"location":"architecture/universal-knowledge-ingestion/#core-tables","title":"Core Tables","text":"<p>tenants <pre><code>CREATE TABLE tenants (\n  id              UUID PRIMARY KEY,\n  slug            TEXT UNIQUE NOT NULL,  -- e.g. 'asha_raj', 'kseeb_grade6'\n  display_name    TEXT NOT NULL,\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipelines \u2013 logical pipelines (e.g. byoeb_kb, shiksha_textbook, seeds_audio) <pre><code>CREATE TABLE pipelines (\n  id              UUID PRIMARY KEY,\n  name            TEXT NOT NULL,           -- 'byoeb_kb', 'shiksha_textbook'\n  domain_profile  TEXT NOT NULL,           -- 'health_byoeb', 'education_shiksha', ...\n  created_at      TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipeline_versions <pre><code>CREATE TABLE pipeline_versions (\n  id                UUID PRIMARY KEY,\n  pipeline_id       UUID REFERENCES pipelines(id),\n  version           TEXT NOT NULL,          -- '0.1', '1.0', '1.1'\n  yaml_profile_name TEXT NOT NULL,          -- name of the YAML in git\n  is_default        BOOLEAN NOT NULL DEFAULT false,\n  created_at        TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>pipeline_runs \u2013 like Shiksha\u2019s pipeline runner, but shared and tenant-aware <pre><code>CREATE TABLE pipeline_runs (\n  id                UUID PRIMARY KEY,\n  tenant_id         UUID REFERENCES tenants(id),\n  pipeline_version_id UUID REFERENCES pipeline_versions(id),\n  source_uri        TEXT NOT NULL,  -- blob path, http URL, etc.\n  status            TEXT NOT NULL,  -- 'queued','running','succeeded','failed'\n  started_at        TIMESTAMPTZ,\n  completed_at      TIMESTAMPTZ,\n  error_message     TEXT\n);\n</code></pre></p> <p>pipeline_step_runs \u2013 thin wrapper around Shiksha\u2019s StepResult <pre><code>CREATE TABLE pipeline_step_runs (\n  id                UUID PRIMARY KEY,\n  pipeline_run_id   UUID REFERENCES pipeline_runs(id),\n  step_name         TEXT NOT NULL,          -- 'text_extraction_llm', 'kb_upload', ...\n  status            TEXT NOT NULL,          -- matches StepStatus\n  output_paths      JSONB,                  -- type-&gt;path (for file-based outputs)\n  metadata          JSONB,\n  error_message     TEXT,\n  started_at        TIMESTAMPTZ,\n  completed_at      TIMESTAMPTZ\n);\n</code></pre></p> <p>knowledge_items \u2013 canonical registry entry <pre><code>CREATE TABLE knowledge_items (\n  id                  UUID PRIMARY KEY,\n  tenant_id           UUID REFERENCES tenants(id),\n  domain_profile      TEXT NOT NULL,\n  modality            TEXT NOT NULL,            -- 'text','document','audio','mixed'\n  source_uri          TEXT NOT NULL,            -- blob/URL/IVR ID\n  pipeline_version_id UUID REFERENCES pipeline_versions(id),\n  title               TEXT,\n  description         TEXT,\n  metadata            JSONB,                    -- board, grade, subject, tags...\n  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),\n  updated_at          TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>chunks \u2013 registry for text/semantic units (BYOEB\u2019s Chunk / LlamaIndex TextNode) <pre><code>CREATE TABLE chunks (\n  id                  TEXT PRIMARY KEY,         -- reuse chunk_id/node_id\n  knowledge_item_id   UUID REFERENCES knowledge_items(id),\n  tenant_id           UUID REFERENCES tenants(id),\n  text                TEXT NOT NULL,\n  metadata            JSONB,\n  vector_store_kind   TEXT NOT NULL,            -- 'azure_vector_search','qdrant',...\n  vector_store_space  TEXT NOT NULL,            -- index/collection/namespace name\n  vector_store_ref_id TEXT NOT NULL,            -- id/primary key inside store\n  created_at          TIMESTAMPTZ NOT NULL DEFAULT now()\n);\n</code></pre></p> <p>We don\u2019t store the vector in Postgres; we just store the pointer to wherever that vector lives.</p> <p>For SEEDS, a <code>KnowledgeItem</code> will additionally carry transcription bits in metadata or a sibling audio_segments table later.</p>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-step-abstractions","title":"Pipeline &amp; Step Abstractions","text":"<p>Step Interface</p> <p>We unify BYOEB\u2019s monolithic <code>KBService.upload()</code> and Shiksha\u2019s multi-step pipeline by introducing a generic Step that always returns a Shiksha-style StepResult. <pre><code>class Step(Protocol):\n    name: str\n\n    async def run(self, ctx: IngestionContext) -&gt; StepResult:\n        ...\n</code></pre></p> <p>Where: <pre><code>@dataclass\nclass StepResult:   # from Shiksha, reused\n    status: StepStatus\n    output_paths: Dict[str, str] | None = None\n    error: Exception | None = None\n    metadata: Dict[str, Any] | None = None\n</code></pre></p> <p><code>IngestionContext</code> is a mutable object holding: - tenant, pipeline_version, pipeline_run_id - references to raw artifacts - intermediate outputs (e.g., extracted markdown path) - a Postgres connection and store registries.</p> <p>Step Types</p> <p>We\u2019ll create adapters so existing code can be treated as Steps.</p> <p>-BYOEB KB Upload Step</p> <p>Wrap the existing <code>KBService.upload()</code> to behave like a pipeline step.</p> <pre><code>class BYOEBKBUploadStep(Step):\n    name = \"byoeb_kb_upload\"\n\n    async def run(self, ctx: IngestionContext) -&gt; StepResult:\n        kb_service = KBService(\n            vector_store=ctx.stores.vector_store,\n            media_storage=ctx.stores.media_storage,\n            llm_client=ctx.models.llm,\n            upsert_t=ctx.config.upsert_threshold,\n        )\n        count = await kb_service.upload(ctx.inputs.files_metadata)\n        return StepResult(\n            status=StepStatus.SUCCESS,\n            metadata={\"chunks_ingested\": count}\n        )\n</code></pre> <p>The orchestrator then records a pipeline_step_runs row with this metadata.</p> <p>-Shiksha Steps</p> <p>Each existing Shiksha Step (TOC, text extraction, text cleaning, LO extraction, index creation, KG) already conforms conceptually: they write outputs to files and can populate <code>StepResult.output_paths</code>.</p> <p>We just ensure: - They accept an <code>IngestionContext</code>. - They fill <code>StepResult</code> and allow the orchestrator to record them.</p> <p>-SEEDS Steps</p> <p>For SEEDS, we\u2019ll add steps like: - <code>AudioDownloadStep</code> (pull from blob/external URL) - <code>TranscriptionStep</code> - <code>KeywordExtractionStep</code> - <code>AudioChunkingStep</code> - <code>EmbeddingIndexingStep</code> - <code>KnowledgeItemRegistrationStep</code></p> <p>All of them return <code>StepResult</code>, write rows to Postgres, and index vectors appropriately.</p>"},{"location":"architecture/universal-knowledge-ingestion/#pipeline-orchestrator","title":"Pipeline Orchestrator","text":"<p>The Ingestion Service runs pipeline versions from YAML configs using these steps. We keep it conceptually simple: <pre><code>class PipelineRunner:\n    async def run(self, tenant, pipeline_version, source_uri, params):\n        run_id = insert_pipeline_run(...)\n        ctx = IngestionContext(...)\n\n        for step in pipeline_version.steps:\n            res = await step.run(ctx)\n            insert_pipeline_step_run(run_id, step.name, res)\n            if res.status is StepStatus.FAILURE:\n                mark_run_failed(run_id, res.error)\n                break\n\n        # On success, create KnowledgeItem rows, Chunk rows, etc.\n</code></pre></p> <p>This is where backward compatibility lives \u2013 old workflows become specific pipeline versions.</p>"},{"location":"architecture/universal-knowledge-ingestion/#future-extensions","title":"Future Extensions","text":"<p>Image &amp; Multimodal Ingestion - Vision OCR for handwritten content and diagrams - Object/diagram extraction and description generation - Multimodal embeddings (e.g., CLIP-style) for image+text retrieval</p> <p>Multi-Lingual Content Handling - Language detection at ingestion time (auto-tag <code>metadata.language</code>) - Language-specific chunking rules (e.g., different tokenization for Hindi/Kannada vs English) - Cross-lingual embeddings or language-specific embedding models - Translation agents for creating parallel content in multiple languages - Script handling (Devanagari, Kannada script) for audio transcription post-processing</p> <p>MCP-Based Ingestion Triggers - Expose ingestion as an MCP (Model Context Protocol) server, allowing LLM agents to directly invoke <code>ingest_content</code> and <code>search_knowledge</code> tools - Enable agentic workflows where an AI assistant can autonomously decide to ingest new content (e.g., \"I found a relevant PDF, let me add it to the knowledge base\") - Support MCP resource discovery \u2014 agents can list available pipelines and their capabilities</p> <p>Cross-Domain Knowledge Graph - Unified graph that connects health, education, and accessibility knowledge - Enable cross-domain queries (e.g., \"health topics relevant to grade 6 science curriculum\") - Shared entity resolution across domains</p> <p>Pipeline Selector Agent - For highly dynamic content (e.g., mixed-format uploads where the optimal pipeline varies), introduce an optional <code>PipelineSelectorAgent</code> - Uses an LLM to analyze incoming content and select the most appropriate pipeline (e.g., \"this looks like a scanned textbook \u2192 use <code>shiksha_textbook_v1</code>\" vs \"this is a plain FAQ document \u2192 use <code>generic_document_v1</code>\") - The selected pipeline still runs statically \u2014 LLM only assists in the initial routing decision - Useful for self-service upload portals where users don't specify content type</p>"}]}